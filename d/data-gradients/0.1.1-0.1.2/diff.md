# Comparing `tmp/data_gradients-0.1.1-py3-none-any.whl.zip` & `tmp/data_gradients-0.1.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,109 +1,121 @@
-Zip file size: 392579 bytes, number of entries: 107
--rw-r--r--  2.0 unx       22 b- defN 23-Jun-26 07:49 data_gradients/__init__.py
--rw-r--r--  2.0 unx      205 b- defN 23-Jun-26 07:49 data_gradients/requirements.txt
--rw-r--r--  2.0 unx      231 b- defN 23-Jun-26 07:49 data_gradients/assets/__init__.py
--rw-r--r--  2.0 unx     2755 b- defN 23-Jun-26 07:49 data_gradients/assets/assets_container.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/assets/css/test.css
--rw-r--r--  2.0 unx     3245 b- defN 23-Jun-26 07:49 data_gradients/assets/html/basic_info_fe.html
--rw-r--r--  2.0 unx     8301 b- defN 23-Jun-26 07:49 data_gradients/assets/html/doc_template.html
--rw-r--r--  2.0 unx      123 b- defN 23-Jun-26 07:49 data_gradients/assets/html/test.html
--rw-r--r--  2.0 unx    51292 b- defN 23-Jun-26 07:49 data_gradients/assets/images/chart_demo.png
--rw-r--r--  2.0 unx   139838 b- defN 23-Jun-26 07:49 data_gradients/assets/images/info.png
--rw-r--r--  2.0 unx    36081 b- defN 23-Jun-26 07:49 data_gradients/assets/images/logo.png
--rw-r--r--  2.0 unx    75086 b- defN 23-Jun-26 07:49 data_gradients/assets/images/warning.png
--rw-r--r--  2.0 unx      333 b- defN 23-Jun-26 07:49 data_gradients/assets/text/lorem_ipsum.txt
--rw-r--r--  2.0 unx       12 b- defN 23-Jun-26 07:49 data_gradients/assets/text/test.txt
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/__init__.py
--rw-r--r--  2.0 unx     1636 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/base.py
--rw-r--r--  2.0 unx     1125 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/detection.py
--rw-r--r--  2.0 unx     1236 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/segmentation.py
--rw-r--r--  2.0 unx     1193 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/adapters/__init__.py
--rw-r--r--  2.0 unx     4621 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/adapters/dataset_adapter.py
--rw-r--r--  2.0 unx     6698 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/adapters/tensor_extractor.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/formatters/__init__.py
--rw-r--r--  2.0 unx      725 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/formatters/base.py
--rw-r--r--  2.0 unx     8757 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/formatters/detection.py
--rw-r--r--  2.0 unx     6940 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/formatters/segmentation.py
--rw-r--r--  2.0 unx     1912 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/formatters/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/preprocessors/__init__.py
--rw-r--r--  2.0 unx      994 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/preprocessors/base.py
--rw-r--r--  2.0 unx     5189 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/preprocessors/contours.py
--rw-r--r--  2.0 unx     2527 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/preprocessors/detection.py
--rw-r--r--  2.0 unx     1836 b- defN 23-Jun-26 07:49 data_gradients/batch_processors/preprocessors/segmentation.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/common/__init__.py
--rw-r--r--  2.0 unx       66 b- defN 23-Jun-26 07:49 data_gradients/common/decorators/__init__.py
--rw-r--r--  2.0 unx     1314 b- defN 23-Jun-26 07:49 data_gradients/common/decorators/decorators.py
--rw-r--r--  2.0 unx      140 b- defN 23-Jun-26 07:49 data_gradients/common/factories/__init__.py
--rw-r--r--  2.0 unx     3694 b- defN 23-Jun-26 07:49 data_gradients/common/factories/base_factory.py
--rw-r--r--  2.0 unx      206 b- defN 23-Jun-26 07:49 data_gradients/common/factories/feature_extractors_factory.py
--rw-r--r--  2.0 unx      569 b- defN 23-Jun-26 07:49 data_gradients/common/factories/list_factory.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/common/registry/__init__.py
--rw-r--r--  2.0 unx     1245 b- defN 23-Jun-26 07:49 data_gradients/common/registry/registry.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/config/__init__.py
--rw-r--r--  2.0 unx      705 b- defN 23-Jun-26 07:49 data_gradients/config/detection.yaml
--rw-r--r--  2.0 unx      756 b- defN 23-Jun-26 07:49 data_gradients/config/segmentation.yaml
--rw-r--r--  2.0 unx     4511 b- defN 23-Jun-26 07:49 data_gradients/config/utils.py
--rw-r--r--  2.0 unx      157 b- defN 23-Jun-26 07:49 data_gradients/config/data/__init__.py
--rw-r--r--  2.0 unx     8713 b- defN 23-Jun-26 07:49 data_gradients/config/data/caching_utils.py
--rw-r--r--  2.0 unx     8640 b- defN 23-Jun-26 07:49 data_gradients/config/data/data_config.py
--rw-r--r--  2.0 unx     2894 b- defN 23-Jun-26 07:49 data_gradients/config/data/questions.py
--rw-r--r--  2.0 unx      231 b- defN 23-Jun-26 07:49 data_gradients/config/data/typing.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/datasets/__init__.py
--rw-r--r--  2.0 unx     2265 b- defN 23-Jun-26 07:49 data_gradients/datasets/bdd_dataset.py
--rw-r--r--  2.0 unx     1565 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/__init__.py
--rw-r--r--  2.0 unx     1176 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/abstract_feature_extractor.py
--rw-r--r--  2.0 unx     4445 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/features.py
--rw-r--r--  2.0 unx     1750 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/utils.py
--rw-r--r--  2.0 unx      325 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/common/__init__.py
--rw-r--r--  2.0 unx     2337 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/common/heatmap.py
--rw-r--r--  2.0 unx     2971 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/common/image_average_brightness.py
--rw-r--r--  2.0 unx     4478 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/common/image_color_distribution.py
--rw-r--r--  2.0 unx     3382 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/common/image_resolution.py
--rw-r--r--  2.0 unx     3219 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/common/sample_visualization.py
--rw-r--r--  2.0 unx     4930 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/common/summary.py
--rw-r--r--  2.0 unx      790 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/object_detection/__init__.py
--rw-r--r--  2.0 unx     2757 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/object_detection/bounding_boxes_area.py
--rw-r--r--  2.0 unx     5663 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/object_detection/bounding_boxes_iou.py
--rw-r--r--  2.0 unx     2382 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/object_detection/bounding_boxes_per_image_count.py
--rw-r--r--  2.0 unx     2877 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/object_detection/bounding_boxes_resolution.py
--rw-r--r--  2.0 unx     2667 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/object_detection/classes_frequency.py
--rw-r--r--  2.0 unx     2815 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/object_detection/classes_frequency_per_image.py
--rw-r--r--  2.0 unx     2479 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/object_detection/classes_heatmap_per_class.py
--rw-r--r--  2.0 unx     1971 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/object_detection/sample_visualization.py
--rw-r--r--  2.0 unx      958 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/__init__.py
--rw-r--r--  2.0 unx     2776 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/bounding_boxes_area.py
--rw-r--r--  2.0 unx     2809 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/bounding_boxes_resolution.py
--rw-r--r--  2.0 unx     2754 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/classes_frequency.py
--rw-r--r--  2.0 unx     2835 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/classes_frequency_per_image.py
--rw-r--r--  2.0 unx     2526 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/classes_heatmap_per_class.py
--rw-r--r--  2.0 unx     2363 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/component_frequency_per_image.py
--rw-r--r--  2.0 unx     2303 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/components_convexity.py
--rw-r--r--  2.0 unx     3808 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/components_erosion.py
--rw-r--r--  2.0 unx     2674 b- defN 23-Jun-26 07:49 data_gradients/feature_extractors/segmentation/sample_visualization.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/managers/__init__.py
--rw-r--r--  2.0 unx    11429 b- defN 23-Jun-26 07:49 data_gradients/managers/abstract_manager.py
--rw-r--r--  2.0 unx     5498 b- defN 23-Jun-26 07:49 data_gradients/managers/detection_manager.py
--rw-r--r--  2.0 unx     5401 b- defN 23-Jun-26 07:49 data_gradients/managers/segmentation_manager.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/utils/__init__.py
--rw-r--r--  2.0 unx     2760 b- defN 23-Jun-26 07:49 data_gradients/utils/detection.py
--rw-r--r--  2.0 unx     1052 b- defN 23-Jun-26 07:49 data_gradients/utils/image_processing.py
--rw-r--r--  2.0 unx     2550 b- defN 23-Jun-26 07:49 data_gradients/utils/pdf_writer.py
--rw-r--r--  2.0 unx     3705 b- defN 23-Jun-26 07:49 data_gradients/utils/summary_writer.py
--rw-r--r--  2.0 unx     2561 b- defN 23-Jun-26 07:49 data_gradients/utils/utils.py
--rw-r--r--  2.0 unx      169 b- defN 23-Jun-26 07:49 data_gradients/utils/common/__init__.py
--rw-r--r--  2.0 unx      249 b- defN 23-Jun-26 07:49 data_gradients/utils/data_classes/__init__.py
--rw-r--r--  2.0 unx      260 b- defN 23-Jun-26 07:49 data_gradients/utils/data_classes/contour.py
--rw-r--r--  2.0 unx     3060 b- defN 23-Jun-26 07:49 data_gradients/utils/data_classes/data_samples.py
--rw-r--r--  2.0 unx     3975 b- defN 23-Jun-26 07:49 data_gradients/utils/data_classes/extractor_results.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-26 07:49 data_gradients/visualize/__init__.py
--rw-r--r--  2.0 unx     5437 b- defN 23-Jun-26 07:49 data_gradients/visualize/detection.py
--rw-r--r--  2.0 unx     5092 b- defN 23-Jun-26 07:49 data_gradients/visualize/images.py
--rw-r--r--  2.0 unx    11756 b- defN 23-Jun-26 07:49 data_gradients/visualize/plot_options.py
--rw-r--r--  2.0 unx    16700 b- defN 23-Jun-26 07:49 data_gradients/visualize/seaborn_renderer.py
--rw-r--r--  2.0 unx    11341 b- defN 23-Jun-26 07:54 data_gradients-0.1.1.dist-info/LICENSE.md
--rw-r--r--  2.0 unx     9226 b- defN 23-Jun-26 07:54 data_gradients-0.1.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-26 07:54 data_gradients-0.1.1.dist-info/WHEEL
--rw-r--r--  2.0 unx       15 b- defN 23-Jun-26 07:54 data_gradients-0.1.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    11250 b- defN 23-Jun-26 07:54 data_gradients-0.1.1.dist-info/RECORD
-107 files, 593382 bytes uncompressed, 373903 bytes compressed:  37.0%
+Zip file size: 415174 bytes, number of entries: 119
+-rw-r--r--  2.0 unx       22 b- defN 23-Jul-10 11:27 data_gradients/__init__.py
+-rw-r--r--  2.0 unx      221 b- defN 23-Jul-10 11:27 data_gradients/requirements.txt
+-rw-r--r--  2.0 unx      231 b- defN 23-Jul-10 11:27 data_gradients/assets/__init__.py
+-rw-r--r--  2.0 unx     2755 b- defN 23-Jul-10 11:27 data_gradients/assets/assets_container.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-10 11:27 data_gradients/assets/css/test.css
+-rw-r--r--  2.0 unx     3244 b- defN 23-Jul-10 11:27 data_gradients/assets/html/basic_info_fe.html
+-rw-r--r--  2.0 unx     8301 b- defN 23-Jul-10 11:27 data_gradients/assets/html/doc_template.html
+-rw-r--r--  2.0 unx      123 b- defN 23-Jul-10 11:27 data_gradients/assets/html/test.html
+-rw-r--r--  2.0 unx    51292 b- defN 23-Jul-10 11:27 data_gradients/assets/images/chart_demo.png
+-rw-r--r--  2.0 unx   139838 b- defN 23-Jul-10 11:27 data_gradients/assets/images/info.png
+-rw-r--r--  2.0 unx    36081 b- defN 23-Jul-10 11:27 data_gradients/assets/images/logo.png
+-rw-r--r--  2.0 unx    75086 b- defN 23-Jul-10 11:27 data_gradients/assets/images/warning.png
+-rw-r--r--  2.0 unx      333 b- defN 23-Jul-10 11:27 data_gradients/assets/text/lorem_ipsum.txt
+-rw-r--r--  2.0 unx       12 b- defN 23-Jul-10 11:27 data_gradients/assets/text/test.txt
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-10 11:27 data_gradients/batch_processors/__init__.py
+-rw-r--r--  2.0 unx     1636 b- defN 23-Jul-10 11:27 data_gradients/batch_processors/base.py
+-rw-r--r--  2.0 unx     1125 b- defN 23-Jul-10 11:27 data_gradients/batch_processors/detection.py
+-rw-r--r--  2.0 unx     1236 b- defN 23-Jul-10 11:27 data_gradients/batch_processors/segmentation.py
+-rw-r--r--  2.0 unx     1155 b- defN 23-Jul-10 11:27 data_gradients/batch_processors/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-10 11:27 data_gradients/batch_processors/adapters/__init__.py
+-rw-r--r--  2.0 unx     4621 b- defN 23-Jul-10 11:27 data_gradients/batch_processors/adapters/dataset_adapter.py
+-rw-r--r--  2.0 unx     6698 b- defN 23-Jul-10 11:27 data_gradients/batch_processors/adapters/tensor_extractor.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-10 11:27 data_gradients/batch_processors/formatters/__init__.py
+-rw-r--r--  2.0 unx      725 b- defN 23-Jul-10 11:27 data_gradients/batch_processors/formatters/base.py
+-rw-r--r--  2.0 unx     9044 b- defN 23-Jul-10 11:27 data_gradients/batch_processors/formatters/detection.py
+-rw-r--r--  2.0 unx     7181 b- defN 23-Jul-10 11:27 data_gradients/batch_processors/formatters/segmentation.py
+-rw-r--r--  2.0 unx     1912 b- defN 23-Jul-10 11:27 data_gradients/batch_processors/formatters/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-10 11:27 data_gradients/batch_processors/preprocessors/__init__.py
+-rw-r--r--  2.0 unx      994 b- defN 23-Jul-10 11:27 data_gradients/batch_processors/preprocessors/base.py
+-rw-r--r--  2.0 unx     5189 b- defN 23-Jul-10 11:27 data_gradients/batch_processors/preprocessors/contours.py
+-rw-r--r--  2.0 unx     2527 b- defN 23-Jul-10 11:27 data_gradients/batch_processors/preprocessors/detection.py
+-rw-r--r--  2.0 unx     1836 b- defN 23-Jul-10 11:27 data_gradients/batch_processors/preprocessors/segmentation.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-10 11:27 data_gradients/common/__init__.py
+-rw-r--r--  2.0 unx       66 b- defN 23-Jul-10 11:27 data_gradients/common/decorators/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 23-Jul-10 11:27 data_gradients/common/decorators/decorators.py
+-rw-r--r--  2.0 unx      140 b- defN 23-Jul-10 11:27 data_gradients/common/factories/__init__.py
+-rw-r--r--  2.0 unx     3694 b- defN 23-Jul-10 11:27 data_gradients/common/factories/base_factory.py
+-rw-r--r--  2.0 unx      206 b- defN 23-Jul-10 11:27 data_gradients/common/factories/feature_extractors_factory.py
+-rw-r--r--  2.0 unx      569 b- defN 23-Jul-10 11:27 data_gradients/common/factories/list_factory.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-10 11:27 data_gradients/common/registry/__init__.py
+-rw-r--r--  2.0 unx     1245 b- defN 23-Jul-10 11:27 data_gradients/common/registry/registry.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-10 11:27 data_gradients/config/__init__.py
+-rw-r--r--  2.0 unx      903 b- defN 23-Jul-10 11:27 data_gradients/config/detection.yaml
+-rw-r--r--  2.0 unx      954 b- defN 23-Jul-10 11:27 data_gradients/config/segmentation.yaml
+-rw-r--r--  2.0 unx     6641 b- defN 23-Jul-10 11:27 data_gradients/config/utils.py
+-rw-r--r--  2.0 unx      157 b- defN 23-Jul-10 11:27 data_gradients/config/data/__init__.py
+-rw-r--r--  2.0 unx     8725 b- defN 23-Jul-10 11:27 data_gradients/config/data/caching_utils.py
+-rw-r--r--  2.0 unx     8676 b- defN 23-Jul-10 11:27 data_gradients/config/data/data_config.py
+-rw-r--r--  2.0 unx     2894 b- defN 23-Jul-10 11:27 data_gradients/config/data/questions.py
+-rw-r--r--  2.0 unx      503 b- defN 23-Jul-10 11:27 data_gradients/config/data/typing.py
+-rw-r--r--  2.0 unx     9524 b- defN 23-Jul-10 11:27 data_gradients/datasets/FolderProcessor.py
+-rw-r--r--  2.0 unx      288 b- defN 23-Jul-10 11:27 data_gradients/datasets/__init__.py
+-rw-r--r--  2.0 unx     2820 b- defN 23-Jul-10 11:27 data_gradients/datasets/base_dataset.py
+-rw-r--r--  2.0 unx     2265 b- defN 23-Jul-10 11:27 data_gradients/datasets/bdd_dataset.py
+-rw-r--r--  2.0 unx      859 b- defN 23-Jul-10 11:27 data_gradients/datasets/utils.py
+-rw-r--r--  2.0 unx      386 b- defN 23-Jul-10 11:27 data_gradients/datasets/detection/__init__.py
+-rw-r--r--  2.0 unx     4037 b- defN 23-Jul-10 11:27 data_gradients/datasets/detection/voc_detection_dataset.py
+-rw-r--r--  2.0 unx     6583 b- defN 23-Jul-10 11:27 data_gradients/datasets/detection/voc_format_detection_dataset.py
+-rw-r--r--  2.0 unx     6987 b- defN 23-Jul-10 11:27 data_gradients/datasets/detection/yolo_format_detection_dataset.py
+-rw-r--r--  2.0 unx     1605 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/__init__.py
+-rw-r--r--  2.0 unx     1461 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/abstract_feature_extractor.py
+-rw-r--r--  2.0 unx     4445 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/features.py
+-rw-r--r--  2.0 unx     4256 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/utils.py
+-rw-r--r--  2.0 unx      394 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/common/__init__.py
+-rw-r--r--  2.0 unx     2337 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/common/heatmap.py
+-rw-r--r--  2.0 unx     3740 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/common/image_average_brightness.py
+-rw-r--r--  2.0 unx     4490 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/common/image_color_distribution.py
+-rw-r--r--  2.0 unx    13444 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/common/image_duplicates.py
+-rw-r--r--  2.0 unx     3362 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/common/image_resolution.py
+-rw-r--r--  2.0 unx     3218 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/common/sample_visualization.py
+-rw-r--r--  2.0 unx     5016 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/common/summary.py
+-rw-r--r--  2.0 unx      790 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/object_detection/__init__.py
+-rw-r--r--  2.0 unx     4124 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/object_detection/bounding_boxes_area.py
+-rw-r--r--  2.0 unx     5754 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/object_detection/bounding_boxes_iou.py
+-rw-r--r--  2.0 unx     2138 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/object_detection/bounding_boxes_per_image_count.py
+-rw-r--r--  2.0 unx     2877 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/object_detection/bounding_boxes_resolution.py
+-rw-r--r--  2.0 unx     4036 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/object_detection/classes_frequency.py
+-rw-r--r--  2.0 unx     4232 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/object_detection/classes_frequency_per_image.py
+-rw-r--r--  2.0 unx     2596 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/object_detection/classes_heatmap_per_class.py
+-rw-r--r--  2.0 unx     1981 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/object_detection/sample_visualization.py
+-rw-r--r--  2.0 unx      958 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/segmentation/__init__.py
+-rw-r--r--  2.0 unx     4209 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/segmentation/bounding_boxes_area.py
+-rw-r--r--  2.0 unx     2809 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/segmentation/bounding_boxes_resolution.py
+-rw-r--r--  2.0 unx     4124 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/segmentation/classes_frequency.py
+-rw-r--r--  2.0 unx     4251 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/segmentation/classes_frequency_per_image.py
+-rw-r--r--  2.0 unx     2645 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/segmentation/classes_heatmap_per_class.py
+-rw-r--r--  2.0 unx     2135 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/segmentation/component_frequency_per_image.py
+-rw-r--r--  2.0 unx     2302 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/segmentation/components_convexity.py
+-rw-r--r--  2.0 unx     3807 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/segmentation/components_erosion.py
+-rw-r--r--  2.0 unx     2674 b- defN 23-Jul-10 11:27 data_gradients/feature_extractors/segmentation/sample_visualization.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-10 11:27 data_gradients/managers/__init__.py
+-rw-r--r--  2.0 unx    12234 b- defN 23-Jul-10 11:27 data_gradients/managers/abstract_manager.py
+-rw-r--r--  2.0 unx     6065 b- defN 23-Jul-10 11:27 data_gradients/managers/detection_manager.py
+-rw-r--r--  2.0 unx     5951 b- defN 23-Jul-10 11:27 data_gradients/managers/segmentation_manager.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-10 11:27 data_gradients/utils/__init__.py
+-rw-r--r--  2.0 unx     2819 b- defN 23-Jul-10 11:27 data_gradients/utils/detection.py
+-rw-r--r--  2.0 unx     1052 b- defN 23-Jul-10 11:27 data_gradients/utils/image_processing.py
+-rw-r--r--  2.0 unx     2550 b- defN 23-Jul-10 11:27 data_gradients/utils/pdf_writer.py
+-rw-r--r--  2.0 unx     3705 b- defN 23-Jul-10 11:27 data_gradients/utils/summary_writer.py
+-rw-r--r--  2.0 unx     2561 b- defN 23-Jul-10 11:27 data_gradients/utils/utils.py
+-rw-r--r--  2.0 unx      169 b- defN 23-Jul-10 11:27 data_gradients/utils/common/__init__.py
+-rw-r--r--  2.0 unx      249 b- defN 23-Jul-10 11:27 data_gradients/utils/data_classes/__init__.py
+-rw-r--r--  2.0 unx      260 b- defN 23-Jul-10 11:27 data_gradients/utils/data_classes/contour.py
+-rw-r--r--  2.0 unx     3088 b- defN 23-Jul-10 11:27 data_gradients/utils/data_classes/data_samples.py
+-rw-r--r--  2.0 unx     3975 b- defN 23-Jul-10 11:27 data_gradients/utils/data_classes/extractor_results.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-10 11:27 data_gradients/visualize/__init__.py
+-rw-r--r--  2.0 unx     5101 b- defN 23-Jul-10 11:27 data_gradients/visualize/images.py
+-rw-r--r--  2.0 unx    11756 b- defN 23-Jul-10 11:27 data_gradients/visualize/plot_options.py
+-rw-r--r--  2.0 unx    16753 b- defN 23-Jul-10 11:27 data_gradients/visualize/seaborn_renderer.py
+-rw-r--r--  2.0 unx     1165 b- defN 23-Jul-10 11:27 data_gradients/visualize/utils.py
+-rw-r--r--  2.0 unx       96 b- defN 23-Jul-10 11:27 data_gradients/visualize/detection/__init__.py
+-rw-r--r--  2.0 unx     4075 b- defN 23-Jul-10 11:27 data_gradients/visualize/detection/detection.py
+-rw-r--r--  2.0 unx     4862 b- defN 23-Jul-10 11:27 data_gradients/visualize/detection/detection_legend.py
+-rw-r--r--  2.0 unx     1432 b- defN 23-Jul-10 11:27 data_gradients/visualize/detection/utils.py
+-rw-r--r--  2.0 unx    11341 b- defN 23-Jul-10 11:32 data_gradients-0.1.2.dist-info/LICENSE.md
+-rw-r--r--  2.0 unx    10637 b- defN 23-Jul-10 11:32 data_gradients-0.1.2.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-10 11:32 data_gradients-0.1.2.dist-info/WHEEL
+-rw-r--r--  2.0 unx       15 b- defN 23-Jul-10 11:32 data_gradients-0.1.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    12527 b- defN 23-Jul-10 11:32 data_gradients-0.1.2.dist-info/RECORD
+119 files, 664594 bytes uncompressed, 394400 bytes compressed:  40.7%
```

## zipnote {}

```diff
@@ -144,20 +144,41 @@
 
 Filename: data_gradients/config/data/questions.py
 Comment: 
 
 Filename: data_gradients/config/data/typing.py
 Comment: 
 
+Filename: data_gradients/datasets/FolderProcessor.py
+Comment: 
+
 Filename: data_gradients/datasets/__init__.py
 Comment: 
 
+Filename: data_gradients/datasets/base_dataset.py
+Comment: 
+
 Filename: data_gradients/datasets/bdd_dataset.py
 Comment: 
 
+Filename: data_gradients/datasets/utils.py
+Comment: 
+
+Filename: data_gradients/datasets/detection/__init__.py
+Comment: 
+
+Filename: data_gradients/datasets/detection/voc_detection_dataset.py
+Comment: 
+
+Filename: data_gradients/datasets/detection/voc_format_detection_dataset.py
+Comment: 
+
+Filename: data_gradients/datasets/detection/yolo_format_detection_dataset.py
+Comment: 
+
 Filename: data_gradients/feature_extractors/__init__.py
 Comment: 
 
 Filename: data_gradients/feature_extractors/abstract_feature_extractor.py
 Comment: 
 
 Filename: data_gradients/feature_extractors/features.py
@@ -174,14 +195,17 @@
 
 Filename: data_gradients/feature_extractors/common/image_average_brightness.py
 Comment: 
 
 Filename: data_gradients/feature_extractors/common/image_color_distribution.py
 Comment: 
 
+Filename: data_gradients/feature_extractors/common/image_duplicates.py
+Comment: 
+
 Filename: data_gradients/feature_extractors/common/image_resolution.py
 Comment: 
 
 Filename: data_gradients/feature_extractors/common/sample_visualization.py
 Comment: 
 
 Filename: data_gradients/feature_extractors/common/summary.py
@@ -288,35 +312,47 @@
 
 Filename: data_gradients/utils/data_classes/extractor_results.py
 Comment: 
 
 Filename: data_gradients/visualize/__init__.py
 Comment: 
 
-Filename: data_gradients/visualize/detection.py
-Comment: 
-
 Filename: data_gradients/visualize/images.py
 Comment: 
 
 Filename: data_gradients/visualize/plot_options.py
 Comment: 
 
 Filename: data_gradients/visualize/seaborn_renderer.py
 Comment: 
 
-Filename: data_gradients-0.1.1.dist-info/LICENSE.md
+Filename: data_gradients/visualize/utils.py
+Comment: 
+
+Filename: data_gradients/visualize/detection/__init__.py
+Comment: 
+
+Filename: data_gradients/visualize/detection/detection.py
+Comment: 
+
+Filename: data_gradients/visualize/detection/detection_legend.py
+Comment: 
+
+Filename: data_gradients/visualize/detection/utils.py
+Comment: 
+
+Filename: data_gradients-0.1.2.dist-info/LICENSE.md
 Comment: 
 
-Filename: data_gradients-0.1.1.dist-info/METADATA
+Filename: data_gradients-0.1.2.dist-info/METADATA
 Comment: 
 
-Filename: data_gradients-0.1.1.dist-info/WHEEL
+Filename: data_gradients-0.1.2.dist-info/WHEEL
 Comment: 
 
-Filename: data_gradients-0.1.1.dist-info/top_level.txt
+Filename: data_gradients-0.1.2.dist-info/top_level.txt
 Comment: 
 
-Filename: data_gradients-0.1.1.dist-info/RECORD
+Filename: data_gradients-0.1.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## data_gradients/__init__.py

```diff
@@ -1 +1 @@
-__version__ = "0.1.1"
+__version__ = "0.1.2"
```

## data_gradients/requirements.txt

```diff
@@ -1,18 +1,19 @@
 hydra-core>=1.2.0
 omegaconf>=2.2.3
 pygments>=2.13.0
 tqdm>=4.64.1
-appdirs>=1.4.4
+platformdirs>=2.5.2
 opencv-python
 Pillow
 tensorboard
 torch
 torchvision
 numpy
 matplotlib
 scipy
 rapidfuzz
 coverage~=5.3.1
 seaborn
 xhtml2pdf
 jinja2
+imagededup
```

## data_gradients/assets/html/basic_info_fe.html

```diff
@@ -11,31 +11,31 @@
             <strong>Validation</strong>
         </th>
     </tr>
     </thead>
     <tbody>
     <tr>
         <td style="text-align:left; color:black;">Images</td>
-        <td class="train_header"><strong>{{train.image_count}}</strong></td>
-        <td class="val_header"><strong>{{val.image_count}}</strong></td>
+        <td class="train_header"><strong>{{train.num_samples}}</strong></td>
+        <td class="val_header"><strong>{{val.num_samples}}</strong></td>
     </tr>
     <tr>
         <td style="text-align:left; color:black;">Classes</td>
         <td class="train_header"><strong>{{train.classes_count}}</strong></td>
         <td class="val_header"><strong>{{val.classes_count}}</strong></td>
     </tr>
     <tr>
         <td style="text-align:left; color:black;">Classes in use</td>
         <td class="train_header"><strong>{{train.classes_in_use}}</strong></td>
         <td class="val_header"><strong>{{val.classes_in_use}}</strong></td>
     </tr>
     <tr>
         <td style="text-align:left; color:black;">Annotations</td>
-        <td class="train_header"><strong>{{train.annotation_count}}</strong></td>
-        <td class="val_header"><strong>{{val.annotation_count}}</strong></td>
+        <td class="train_header"><strong>{{train.num_annotations}}</strong></td>
+        <td class="val_header"><strong>{{val.num_annotations}}</strong></td>
     </tr>
     <tr>
         <td style="text-align:left; color:black;">Annotations per images</td>
         <td class="train_text"><strong>{{train.annotations_per_image}}</strong></td>
         <td class="val_text"><strong>{{val.annotations_per_image}}</strong></td>
     </tr>
     <tr>
@@ -66,8 +66,8 @@
     <tr>
         <td style="text-align:left; color:black;">Least annotations in an image</td>
         <td class="train_text"><strong>{{train.least_annotations}}</strong></td>
         <td class="val_text"><strong>{{val.least_annotations}}</strong></td>
     </tr>
 
     </tbody>
-</table>
+</table>
```

### html2text {}

```diff
@@ -1,14 +1,14 @@
 *****      Train                              Validation
 *****
-Images      {{train.image_count}}              {{val.image_count}}
+Images      {{train.num_samples}}              {{val.num_samples}}
 Classes     {{train.classes_count}}            {{val.classes_count}}
 Classes in  {{train.classes_in_use}}           {{val.classes_in_use}}
 use
-Annotations {{train.annotation_count}}         {{val.annotation_count}}
+Annotations {{train.num_annotations}}          {{val.num_annotations}}
 Annotations {{train.annotations_per_image}}    {{val.annotations_per_image}}
 per images
 Images with {                                  {
 no          {train.images_without_annotation}} {val.images_without_annotation}}
 annotations
 Median
 image       {{train.med_image_resolution}}     {{val.med_image_resolution}}
```

## data_gradients/batch_processors/utils.py

```diff
@@ -1,8 +1,7 @@
-import numpy as np
 import torch
 
 
 def channels_last_to_first(tensors: torch.Tensor) -> torch.Tensor:
     """
     Permute BS, W, H, C -> BS, C, H, W
             0   1  2  3 -> 0   3  1  2
@@ -13,22 +12,24 @@
 
 
 def check_all_integers(tensor: torch.Tensor) -> bool:
     return torch.all(torch.eq(tensor, tensor.to(torch.int))).item()
 
 
 def to_one_hot(labels: torch.Tensor, n_classes: int) -> torch.Tensor:
-    """Method gets label with the shape of [BS, N, H, W] where N is the number of classes.
-    This numpy implementation is much faster than Torch.nn.functional.one_hot.
+    """
+    Converts labels to one-hot encoded representation.
+
     :param labels:      Tensor of shape [BS, H, W]
     :param n_classes:   Number of classes in the dataset.
     :return:            Labels tensor shaped as [BS, N, H, W]
     """
+    batch_size, height, width = labels.shape
+
+    # Expand dimensions and create a tensor filled with zeros
+    labels_one_hot = torch.zeros(batch_size, n_classes, height, width, device=labels.device)
+
+    # Set corresponding class index to 1 in one-hot representation
+    labels_long = labels.long()  # Convert labels to long (int64) dtype
+    labels_one_hot.scatter_(1, labels_long.unsqueeze(1), 1)
 
-    labels = labels.to(torch.int64)
-    labels_np = labels.numpy()
-    out = np.zeros((labels_np.size, n_classes), dtype=np.uint8)
-    out[np.arange(labels_np.size), labels_np.ravel()] = 1
-    out.shape = labels_np.shape + (n_classes,)
-    labels = torch.from_numpy(out)
-    labels = labels.squeeze().permute(0, -1, 1, 2)
-    return labels
+    return labels_one_hot
```

## data_gradients/batch_processors/formatters/detection.py

```diff
@@ -53,14 +53,18 @@
         :param images: Batch of images, in (BS, ...) format
         :param labels: Batch of labels, in (BS, N, 5) format
         :return:
             - images: Batch of images already formatted into (BS, C, H, W)
             - labels: List of bounding boxes, each of shape (N_i, 5 [label_xyxy]) with N_i being the number of bounding boxes with class_id in class_ids
         """
 
+        # Might happen if the user passes tensors as [N, 5] with N=1; Depending on the Dataset implementation, it may actually return a [5] tensor instead
+        if labels.numel() == 0:
+            labels = torch.zeros((0, 5))
+
         # If the label is of shape [N, 5] we can assume that it represents the targets of a single sample (class_name + 4 bbox coordinates)
         if labels.ndim == 2 and labels.shape[1] == 5:
             images = images.unsqueeze(0)
             labels = labels.unsqueeze(0)
 
         labels = drop_nan(labels)
 
@@ -72,22 +76,22 @@
         self.label_first = self.data_config.get_is_label_first(hint=targets_sample_str)
         self.xyxy_converter = self.data_config.get_xyxy_converter(hint=targets_sample_str)
 
         if 0 <= images.min() and images.max() <= 1:
             images *= 255
             images = images.to(torch.uint8)
 
-        labels = self.convert_to_label_xyxy(
-            annotated_bboxes=labels,
-            image_shape=images.shape[-2:],
-            xyxy_converter=self.xyxy_converter,
-            label_first=self.label_first,
-        )
-
-        labels = self.filter_non_relevant_annotations(bboxes=labels, class_ids_to_use=self.class_ids_to_use)
+        if labels.numel() > 0:
+            labels = self.convert_to_label_xyxy(
+                annotated_bboxes=labels,
+                image_shape=images.shape[-2:],
+                xyxy_converter=self.xyxy_converter,
+                label_first=self.label_first,
+            )
+            labels = self.filter_non_relevant_annotations(bboxes=labels, class_ids_to_use=self.class_ids_to_use)
 
         return images, labels
 
     @staticmethod
     def ensure_labels_shape(annotated_bboxes: Tensor) -> Tensor:
         """Make sure that the labels have the correct shape, i.e. (BS, N, 5)."""
         if annotated_bboxes.ndim == 2:
```

## data_gradients/batch_processors/formatters/segmentation.py

```diff
@@ -76,27 +76,32 @@
         images = drop_nan(images)
         labels = drop_nan(labels)
 
         images = ensure_channel_first(images, n_image_channels=self.n_image_channels)
         labels = ensure_channel_first(labels, n_image_channels=self.n_image_channels)
 
         images = ensure_images_shape(images, n_image_channels=self.n_image_channels)
-        labels = self.ensure_labels_shape(labels, n_classes=self.n_image_channels, ignore_labels=self.ignore_labels)
+        labels = self.validate_labels_dim(labels, n_classes=self.n_image_channels, ignore_labels=self.ignore_labels)
 
         labels = self.ensure_hard_labels(labels, n_classes=len(self.class_names), threshold_value=self.threshold_value)
 
         if self.require_onehot(labels=labels, n_classes=len(self.class_names)):
             labels = to_one_hot(labels, n_classes=len(self.class_names))
 
         for class_id_to_ignore in self.class_ids_to_ignore:
             labels[:, class_id_to_ignore, ...] = 0
 
         if 0 <= images.min() and images.max() <= 1:
             images *= 255
             images = images.to(torch.uint8)
+        elif images.min() < 0:  # images were normalized with some unknown mean and std
+            images -= images.min()
+            images /= images.max()
+            images *= 255
+            images = images.to(torch.uint8)
 
         return images, labels
 
     @staticmethod
     def ensure_hard_labels(labels: Tensor, n_classes: int, threshold_value: float) -> Tensor:
         unique_values = torch.unique(labels)
 
@@ -122,35 +127,34 @@
     @staticmethod
     def require_onehot(labels: Tensor, n_classes: int) -> bool:
         is_binary = n_classes == 1
         is_onehot = labels.shape[1] == n_classes
         return not (is_binary or is_onehot)
 
     @staticmethod
-    def ensure_labels_shape(labels: Tensor, n_classes: int, ignore_labels: List[int]) -> Tensor:
+    def validate_labels_dim(labels: Tensor, n_classes: int, ignore_labels: List[int]) -> Tensor:
         """
         Validating labels dimensions are (BS, N, H, W) where N is either 1 or number of valid classes
-        :param labels: Tensor [BS, N, W, H]
-        :return: labels: Tensor [BS, N, W, H]
+        :param labels:      Tensor [BS, W, H] or [BS, N, W, H]
+        :return: labels:    Tensor [BS, N, W, H]
         """
         if labels.dim() == 3:
-            labels = labels.unsqueeze(1)  # Probably (B, H, W)
-            return labels
+            return labels  # Assuming [BS, W, H]
         elif labels.dim() == 4:
             total_n_classes = n_classes + len(ignore_labels)
             valid_n_classes = (total_n_classes, 1)
             input_n_classes = labels.shape[1]
             if input_n_classes not in valid_n_classes and labels.shape[-1] not in valid_n_classes:
                 raise DatasetFormatError(
                     f"Labels batch shape should be [BS, N, W, H] where N is either 1 or n_classes + len(ignore_labels)"
                     f" ({total_n_classes}). Got: {input_n_classes}"
                 )
             return labels
         else:
-            raise DatasetFormatError(f"Labels batch shape should be [BatchSize x Channels x Width x Height]. Got {labels.shape}")
+            raise DatasetFormatError(f"Labels batch shape should be [Channels x Width x Height] or [BatchSize x Channels x Width x Height]. Got {labels.shape}")
 
     @staticmethod
     def binary_mask_above_threshold(labels: Tensor, threshold_value: float) -> Tensor:
         # Support only for binary segmentation
         labels = torch.where(
             labels > threshold_value,
             torch.ones_like(labels),
```

## data_gradients/config/detection.yaml

```diff
@@ -11,15 +11,21 @@
           n_rows: 3
           n_cols: 4
           stack_splits_vertically: True
       - DetectionClassHeatmap:
           n_rows: 6
           n_cols: 2
           heatmap_shape: [200, 200]
-      - DetectionBoundingBoxArea
+      - DetectionBoundingBoxArea:
+          topk: 30
+          prioritization_mode: train_val_diff
       - DetectionBoundingBoxPerImageCount
       - DetectionBoundingBoxSize
-      - DetectionClassFrequency
-      - DetectionClassesPerImageCount
+      - DetectionClassFrequency:
+          topk: 30
+          prioritization_mode: train_val_diff
+      - DetectionClassesPerImageCount:
+          topk: 30
+          prioritization_mode: train_val_diff
       - DetectionBoundingBoxIoU:
           num_bins: 10
           class_agnostic: true
```

## data_gradients/config/segmentation.yaml

```diff
@@ -12,14 +12,20 @@
           n_cols: 3
           stack_splits_vertically: True
           stack_mask_vertically: True
       - SegmentationClassHeatmap:
           n_rows: 6
           n_cols: 2
           heatmap_shape: [200, 200]
-      - SegmentationClassFrequency
-      - SegmentationClassesPerImageCount
+      - SegmentationClassFrequency:
+          topk: 30
+          prioritization_mode: train_val_diff
+      - SegmentationClassesPerImageCount:
+          topk: 30
+          prioritization_mode: train_val_diff
       - SegmentationComponentsPerImageCount
       - SegmentationBoundingBoxResolution
-      - SegmentationBoundingBoxArea
+      - SegmentationBoundingBoxArea:
+          topk: 30
+          prioritization_mode: train_val_diff
       - SegmentationComponentsConvexity
       - SegmentationComponentsErosion
```

## data_gradients/config/utils.py

```diff
@@ -2,14 +2,15 @@
 from typing import Optional, Dict, Any, List, Tuple
 
 import hydra
 from hydra import initialize_config_dir, compose
 from hydra.core.global_hydra import GlobalHydra
 from omegaconf import DictConfig, OmegaConf
 
+from data_gradients.config.data.typing import FeatureExtractorsType
 from data_gradients.feature_extractors import AbstractFeatureExtractor
 from data_gradients.common.factories import FeatureExtractorsFactory, ListFactory
 
 OmegaConf.register_new_resolver("merge", lambda x, y: x + y)
 
 
 def load_report_feature_extractors(
@@ -28,14 +29,61 @@
     grouped_feature_extractors = {}
     for section in cfg["report_sections"]:
         section_name, feature_extractors = section["name"], section["features"]
         grouped_feature_extractors[section_name] = ListFactory(FeatureExtractorsFactory()).get(feature_extractors)
     return grouped_feature_extractors
 
 
+
+def get_grouped_feature_extractors(
+    default_config_name: str,
+    config_path: str,
+    feature_extractors: FeatureExtractorsType,
+) -> Dict[str, List[AbstractFeatureExtractor]]:
+    if feature_extractors is None:
+        if config_path is None:
+            config_dir, config_name = None, default_config_name
+        else:
+            config_path = os.path.abspath(config_path)
+            config_dir, config_name = (
+                os.path.dirname(config_path),
+                os.path.basename(config_path).split(".")[0],
+            )
+        grouped_feature_extractors = load_report_feature_extractors(
+            config_name=config_name, config_dir=config_dir
+        )
+    else:
+        if not isinstance(feature_extractors, list):
+            feature_extractors = [feature_extractors]
+
+        section_name = "Selected features"
+        grouped_feature_extractors = {section_name: []}
+        for feature_extractor in feature_extractors:
+            if isinstance(feature_extractor, AbstractFeatureExtractor):
+                grouped_feature_extractors[section_name].append(feature_extractor)
+            elif isinstance(feature_extractor, str):
+                grouped_feature_extractors[section_name].append(
+                    FeatureExtractorsFactory().get(feature_extractor)
+                )
+            elif issubclass(feature_extractor, AbstractFeatureExtractor):
+                try:
+                    grouped_feature_extractors[section_name].append(feature_extractor())
+                except RuntimeError as e:
+                    raise RuntimeError(
+                        f"The feature extractor {feature_extractor.__class__} requires additional init argument. "
+                        f"Initialize the feature extractor and pass it as an instance"
+                    ) from e
+            else:
+                raise TypeError(
+                    f"Unsupported feature extractor type. Supported types are string (name of FeatureExtractor) or AbstractFeatureExtractor"
+                )
+
+    return grouped_feature_extractors
+
+
 def load_config(config_name: str, config_dir: str, overrides: Optional[Dict[str, Any]] = None) -> DictConfig:
     """Load a Hydra configuration file and instantiate it.
 
     :param config_name: Name of the Hydra configuration file to load.
     :param config_dir:  Directory where the Hydra configuration file is located. By default, uses the package config directory.
     :param overrides:   Dictionary with overrides for the configuration file. By default, no overrides will be applied.
     :return:            An instantiated configuration object.
```

## data_gradients/config/data/caching_utils.py

```diff
@@ -51,20 +51,20 @@
 
         :param tensor_extractor: Either a string representation (e.g. `[1].bbox`) or a custom callable (e.g. lambda x: x[1]["bbox"])
         :return: Tensor extractor, extracting a specific tensor from the dataset outputs.
         """
         return TensorExtractorResolver._resolve(tensor_extractor).value
 
     @staticmethod
-    def to_string(tensor_extractor: Union[None, str, Callable[[SupportedDataType], torch.Tensor]]) -> str:
+    def to_string(tensor_extractor: Union[None, str, Callable[[SupportedDataType], torch.Tensor]]) -> Optional[str]:
         """Ensures the input `tensor_extractor` to be a string.
 
         For example:
             >> TensorExtractorResolver.to_string(None)
-            # "None"
+            # None
 
             >> TensorExtractorResolver.to_string("[1].bbox")
             # "[1].bbox"
 
             >> TensorExtractorResolver.to_string(lambda x: x[1]["bbox"])
             # [Non-cachable] - function <lambda> at 0x102ca58b0
 
@@ -88,15 +88,15 @@
             >> TensorExtractorResolver.resolve(lambda x: x[1]["bbox"])
             # CachableParam(value=lambda x: x[1]["bbox"], name="[Non-cachable] - function <lambda> at 0x102ca58b0")
 
         :param tensor_extractor: Either None, a string representation (e.g. `[1].bbox`) or a custom callable (e.g. lambda x: x[1]["bbox"])
         :return: Dataclass including both the value (used in the code) and the name (used in the cache) of this function.
         """
         if tensor_extractor is None:
-            return CachableParam(value=None, name="None")
+            return CachableParam(value=None, name=None)
 
         elif isinstance(tensor_extractor, str):
             if tensor_extractor.startswith(NON_CACHABLE_PREFIX):
                 # The value corresponds to the cache of a custom function. THis means that the function was cached by user in previous run,
                 # but he did not provide a function for this run.
                 # Since we cannot build back the original function, we raise an informative exception.
                 raise CacheLoadingError(key="tensor_extractor", value=tensor_extractor)
@@ -122,20 +122,20 @@
 
         :param xyxy_converter: Either a string representation (e.g. `xywh`) or a custom callable (e.g. custom_xywh2xyxy or lambda bbox: ...)
         :return: Callable converting bboxes into xyxy.
         """
         return XYXYConverterResolver._resolve(xyxy_converter).value
 
     @staticmethod
-    def to_string(xyxy_converter: Union[None, str, Callable[[torch.Tensor], torch.Tensor]]) -> str:
+    def to_string(xyxy_converter: Union[None, str, Callable[[torch.Tensor], torch.Tensor]]) -> Optional[str]:
         """Ensures the input `xyxy_converter` to be a string.
 
         For example:
             >> XYXYConverterResolver.to_string(None)
-            # "None"
+            # None
 
             >> XYXYConverterResolver.to_string("xywh")
             # "xyxy"
 
             >> XYXYConverterResolver.to_string(custom_xywh2xyxy)
             # [Non-cachable] - <function custom_xywh2xyxy at 0x102ca5820>
 
@@ -157,15 +157,15 @@
             >> XYXYConverterResolver.resolve(custom_xywh2xyxy)
             # CachableParam(value=custom_xywh2xyxy, name="[Non-cachable] - <function custom_xywh2xyxy at 0x102ca5820>")
 
         :param xyxy_converter: Either None, a string representation (e.g. `xywh`) or a custom callable (e.g. custom_xywh2xyxy or lambda bbox: ...)
         :return: Dataclass including both the value (used in the code) and the name (used in the cache) of this function.
         """
         if xyxy_converter is None:
-            return CachableParam(value=None, name="None")
+            return CachableParam(value=None, name=None)
 
         elif isinstance(xyxy_converter, str):
             if xyxy_converter.startswith(NON_CACHABLE_PREFIX):
                 # The value corresponds to the cache of a custom function. THis means that the function was cached by user in previous run,
                 # but he did not provide a function for this run.
                 # Since we cannot build back the original function, we raise an informative exception.
                 raise CacheLoadingError(key="xyxy_converter", value=xyxy_converter)
```

## data_gradients/config/data/data_config.py

```diff
@@ -1,11 +1,12 @@
 import os
 import logging
+
+import platformdirs
 import torch
-import appdirs
 from abc import ABC
 from dataclasses import dataclass, field
 from typing import Dict, Optional, Callable, Union
 
 import data_gradients
 from data_gradients.config.data.questions import Question, ask_question, text_to_yellow
 from data_gradients.config.data.caching_utils import TensorExtractorResolver, XYXYConverterResolver
@@ -27,15 +28,15 @@
             Also supports saving and loading from callable defined within DataGradients.
     """
 
     use_cache: bool = False  # To avoid user facing unexpected errors asking for nothing. Examples should set it to True.
     images_extractor: Union[None, str, Callable[[SupportedDataType], torch.Tensor]] = None
     labels_extractor: Union[None, str, Callable[[SupportedDataType], torch.Tensor]] = None
 
-    DEFAULT_CACHE_DIR: str = field(default=appdirs.user_cache_dir("DataGradients", "Deci"), init=False)
+    DEFAULT_CACHE_DIR: str = field(default=platformdirs.user_cache_dir("DataGradients", "Deci"), init=False)
 
     @classmethod
     def load_from_json(cls, filename: str, dir_path: Optional[str] = None) -> "DataConfig":
         """Load an instance of DataConfig directly from a cache file.
         :param filename: Name of the cache file. This should include ".json" extension.
         :param dir_path: Path to the folder where the cache file is located. By default, the cache file will be loaded from the user cache directory.
         :return: An instance of DataConfig loaded from the cache file.
@@ -95,15 +96,15 @@
         dir_path = cache_dir_path or self.DEFAULT_CACHE_DIR
         path = os.path.join(dir_path, cache_filename)
         if self.use_cache:
             cache_dict = self._load_json_dict(path=path)
             if cache_dict:
                 logger.info(
                     f"Cache activated for `{self.__class__.__name__}`. This will be used to set attributes that you did not set manually. "
-                    f"Please set `use_cache=False` if you want to deactivate it."
+                    f'Caching path = "{path}". Please set `use_cache=False` if you want to deactivate it.'
                 )
                 self._fill_missing_params(json_dict=cache_dict)
         else:
             logger.info(f"Cache deactivated for `{self.__class__.__name__}`. Please set `load_cache=True` if you want to activate it.")
 
     def _fill_missing_params(self, json_dict: JSONDict):
         """Overwrite every attribute that is equal to `None`.
```

## data_gradients/config/data/typing.py

```diff
@@ -1,5 +1,15 @@
-from typing import Union, Tuple, List, Mapping, Dict
+from typing import Union, Tuple, List, Mapping, Dict, Type
+
+from data_gradients.feature_extractors import AbstractFeatureExtractor
 
 SupportedDataType = Union[Tuple, List, Mapping]
-JSONValue = Union[str, int, float, bool, None, Dict[str, Union["JSONValue", List["JSONValue"]]]]
+JSONValue = Union[
+    str, int, float, bool, None, Dict[str, Union["JSONValue", List["JSONValue"]]]
+]
 JSONDict = Dict[str, JSONValue]
+FeatureExtractorsType = Union[
+    List[Union[str, AbstractFeatureExtractor, Type[AbstractFeatureExtractor]]],
+    str,
+    AbstractFeatureExtractor,
+    Type[AbstractFeatureExtractor],
+]
```

## data_gradients/datasets/__init__.py

```diff
@@ -0,0 +1,18 @@
+00000000: 6672 6f6d 2064 6174 615f 6772 6164 6965  from data_gradie
+00000010: 6e74 732e 6461 7461 7365 7473 2e64 6574  nts.datasets.det
+00000020: 6563 7469 6f6e 2069 6d70 6f72 7420 564f  ection import VO
+00000030: 4344 6574 6563 7469 6f6e 4461 7461 7365  CDetectionDatase
+00000040: 742c 2056 4f43 466f 726d 6174 4465 7465  t, VOCFormatDete
+00000050: 6374 696f 6e44 6174 6173 6574 2c20 596f  ctionDataset, Yo
+00000060: 6c6f 466f 726d 6174 4465 7465 6374 696f  loFormatDetectio
+00000070: 6e44 6174 6173 6574 0a66 726f 6d20 6461  nDataset.from da
+00000080: 7461 5f67 7261 6469 656e 7473 2e64 6174  ta_gradients.dat
+00000090: 6173 6574 732e 6264 645f 6461 7461 7365  asets.bdd_datase
+000000a0: 7420 696d 706f 7274 2042 4444 4461 7461  t import BDDData
+000000b0: 7365 740a 0a5f 5f61 6c6c 5f5f 203d 205b  set..__all__ = [
+000000c0: 2256 4f43 4465 7465 6374 696f 6e44 6174  "VOCDetectionDat
+000000d0: 6173 6574 222c 2022 564f 4346 6f72 6d61  aset", "VOCForma
+000000e0: 7444 6574 6563 7469 6f6e 4461 7461 7365  tDetectionDatase
+000000f0: 7422 2c20 2259 6f6c 6f46 6f72 6d61 7444  t", "YoloFormatD
+00000100: 6574 6563 7469 6f6e 4461 7461 7365 7422  etectionDataset"
+00000110: 2c20 2242 4444 4461 7461 7365 7422 5d0a  , "BDDDataset"].
```

## data_gradients/feature_extractors/__init__.py

```diff
@@ -1,9 +1,9 @@
 from .abstract_feature_extractor import AbstractFeatureExtractor
-from .common import ImagesAverageBrightness, ImageColorDistribution, ImagesResolution, SummaryStats
+from .common import ImagesAverageBrightness, ImageColorDistribution, ImagesResolution, SummaryStats, ImageDuplicates
 from .segmentation import (
     SegmentationBoundingBoxArea,
     SegmentationBoundingBoxResolution,
     SegmentationClassFrequency,
     SegmentationClassHeatmap,
     SegmentationClassesPerImageCount,
     SegmentationComponentsConvexity,
@@ -19,14 +19,15 @@
     DetectionClassHeatmap,
     DetectionClassesPerImageCount,
     DetectionSampleVisualization,
     DetectionBoundingBoxIoU,
 )
 
 __all__ = [
+    "ImageDuplicates",
     "AbstractFeatureExtractor",
     "ImagesAverageBrightness",
     "ImageColorDistribution",
     "ImagesResolution",
     "SummaryStats",
     "SegmentationBoundingBoxArea",
     "SegmentationBoundingBoxResolution",
```

## data_gradients/feature_extractors/abstract_feature_extractor.py

```diff
@@ -1,10 +1,10 @@
 from abc import ABC, abstractmethod
 from dataclasses import dataclass
-from typing import Union, Optional
+from typing import Union, Optional, Iterable
 
 import matplotlib.pyplot as plt
 import numpy as np
 import pandas as pd
 
 from data_gradients.utils.data_classes.data_samples import ImageSample
 from data_gradients.visualize.plot_options import CommonPlotOptions
@@ -42,9 +42,16 @@
     def notice(self) -> Optional[str]:
         return None
 
     @property
     def warning(self) -> Optional[str]:
         return None
 
+    def setup_data_sources(self, tran_data: Iterable, val_data: Iterable):
+        """
+        Called in AnalysisManagerAbstract.__init__ for the purpose of exposing tran_data and val_data
+         to the feature in case some information is needed.
+        """
+        pass
+
     def __repr__(self):
         return self.__class__.__name__
```

## data_gradients/feature_extractors/utils.py

```diff
@@ -1,40 +1,75 @@
-from typing import Dict, List, Any, Tuple
+import pandas as pd
 
-import numpy as np
 
-
-def align_histogram_keys(train_histogram: Dict[str, Any], val_histogram: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
-    """Enforces the keys of training and validation histograms to be the same.
-    If one of the keys is missing, the histogram will filled with defaults value (0, 0.0, "") depending on the situation
-
-    :param train_histogram:  Histogram representing metrics from training split.
-    :param val_histogram:  Histogram representing metrics from validation split.
-    :return: A merged dictionary containing key-value pairs from both "train" and "val" splits.
-    """
-    keys = set(train_histogram.keys()) | set(val_histogram.keys())
-
-    aligned_train_histogram, aligned_val_histogram = {}, {}
-    for key in keys:
-        train_value = train_histogram.get(key)
-        val_value = val_histogram.get(key)
-
-        value_type = type(train_value) if train_value is not None else type(val_value)
-        default_value = value_type()
-
-        aligned_train_histogram[key] = train_value or default_value
-        aligned_val_histogram[key] = val_value or default_value
-
-    return aligned_train_histogram, aligned_val_histogram
-
-
-def normalize_values_to_percentages(counters: List[float], total_count: float) -> List[float]:
-    """
-    Normalize a list of count to percentages relative to a total value.
-
-    :param counters:    Values to normalize.
-    :param total_count: Total number of values, which will be used to calculate percentages.
-    :return:            Values representing the percentages of each input value.
-    """
-    if total_count == 0:
-        total_count = 1
-    return [np.round(((100 * count) / total_count), 3) for count in counters]
+class MostImportantValuesSelector:
+    def __init__(self, topk: int, prioritization_mode: str):
+        """
+        :param topk:    How many rows (per split) to return.
+        :param prioritization_mode:    The prioritization_mode to get the top values for. One of:
+                - 'train_val_diff': Returns the top k rows with the biggest train_val_diff between 'train' and 'val' split values.
+                - 'outliers':       Returns the top k rows with the most extreme average values.
+                - 'max':            Returns the top k rows with the highest average values.
+                - 'min':            Returns the top k rows with the lowest average values.
+                - 'min_max':        Returns the (top k)/2 rows with the biggest average values, and the (top k)/2 with the smallest average values.
+        """
+        valid_modes = ("train_val_diff", "outliers", "max", "min", "min_max")
+        if prioritization_mode not in valid_modes:
+            raise ValueError(f"Invalid `prioritization_mode={prioritization_mode}'. Must be one of: {valid_modes}.")
+        self.topk = topk
+        self.prioritization_mode = prioritization_mode
+
+    def select(self, df: pd.DataFrame, id_col: str, split_col: str, value_col: str):
+        """
+        Returns the top k rows of the DataFrame based on the prioritization_mode.
+        The DataFrame is expected to have at least three columns: id_col, split_col, val_col.
+
+        :param df:          The DataFrame to get the top values from.
+        :param id_col:      The name of the id column.
+        :param split_col:   The name of the split column. (Usually 'split')
+        :param value_col:   The name of column that will be used to calculate the metric.
+        :return: The Dataframe with only the rows associated to the most important values.
+        """
+        # Verify inputs
+        for col in [id_col, split_col, value_col]:
+            if col not in df.columns:
+                raise ValueError(f"{col} is not a column in the DataFrame")
+
+        # the mean of val_col for each id_col/split_col
+        df_mean = df.groupby([id_col, split_col])[value_col].mean().reset_index()
+
+        # Pivot DataFrame to have 'train' and 'val' as columns
+        df_pivot = df_mean.pivot(index=id_col, columns=split_col, values=value_col)
+
+        # Calculate the relative difference or average based on the prioritization_mode
+        if self.prioritization_mode == "train_val_diff":
+            # `train_val_diff` only defined when working with 2 sets.
+            if len(df_pivot.columns) != 2:
+                raise ValueError(f'`prioritization_mode"train_val_diff"` is only supported when working with 2 sets. Found {len(df_pivot.columns)}.')
+            delta = (df_pivot.iloc[:, 0] - df_pivot.iloc[:, 1]).abs()
+            average = (df_pivot.iloc[:, 0] + df_pivot.iloc[:, 1]).abs() / 2
+            df_pivot["metric"] = delta / (average + 1e-6)
+        elif self.prioritization_mode in ["outliers", "max", "min", "min_max"]:
+            df_pivot["metric"] = df_pivot.mean(1)
+
+        if self.prioritization_mode == "outliers":
+            mean, std = df_pivot["metric"].mean(), df_pivot["metric"].std()
+            df_pivot["metric"] = (df_pivot["metric"] - mean).abs() / (std + 1e-6)
+
+        # Only return the top k.
+        if self.prioritization_mode in ["train_val_diff", "outliers", "max"]:
+            top_ids = df_pivot.nlargest(self.topk, "metric").index
+            return df[df[id_col].isin(top_ids)]
+        elif self.prioritization_mode == "min":
+            top_ids = df_pivot.nsmallest(self.topk, "metric").index
+            return df[df[id_col].isin(top_ids)]
+        elif self.prioritization_mode == "min_max":
+            n_max_results = self.topk // 2
+            n_min_results = self.topk - n_max_results
+
+            top_ids = df_pivot.nlargest(n_max_results, "metric").index
+
+            n_rows_available = len(df_pivot) - len(top_ids)
+            bottom_ids = df_pivot.nsmallest(min(n_min_results, n_rows_available), "metric").index
+            return pd.concat([df[df[id_col].isin(top_ids)], df[df[id_col].isin(bottom_ids)]])
+        else:
+            raise NotImplementedError(f"Mode {self.prioritization_mode} is not implemented")
```

## data_gradients/feature_extractors/common/__init__.py

```diff
@@ -1,12 +1,14 @@
 from .image_average_brightness import ImagesAverageBrightness
 from .image_color_distribution import ImageColorDistribution
+from .image_duplicates import ImageDuplicates
 from .image_resolution import ImagesResolution
 from .summary import SummaryStats
 
 
 __all__ = [
+    "ImageDuplicates",
     "ImagesAverageBrightness",
     "ImageColorDistribution",
     "ImagesResolution",
     "SummaryStats",
 ]
```

## data_gradients/feature_extractors/common/image_average_brightness.py

```diff
@@ -2,14 +2,15 @@
 import numpy as np
 import pandas as pd
 
 from data_gradients.common.registry.registry import register_feature_extractor
 from data_gradients.feature_extractors.abstract_feature_extractor import AbstractFeatureExtractor
 from data_gradients.utils.data_classes.data_samples import ImageSample, ImageChannelFormat
 from data_gradients.visualize.plot_options import KDEPlotOptions
+from data_gradients.visualize.plot_options import BarPlotOptions
 from data_gradients.feature_extractors.abstract_feature_extractor import Feature
 
 
 @register_feature_extractor()
 class ImagesAverageBrightness(AbstractFeatureExtractor):
     """Extracts the distribution of the image 'brightness'."""
 
@@ -38,26 +39,41 @@
         else:
             raise ValueError(f"Unknown image format {sample.image_format}")
 
         self.data.append({"split": sample.split, "brightness": brightness})
 
     def aggregate(self) -> Feature:
         df = pd.DataFrame(self.data)
+        n_unique_per_split = {len(df[df["split"] == split]["brightness"].unique()) for split in df["split"].unique()}
+
+        # If a split has only one unique value, KDE plot will not work. Instead, we show the average brightness of the images.
+        if 1 in n_unique_per_split:
+            plot_options = BarPlotOptions(
+                x_label_key="split",
+                x_label_name="Split",
+                y_label_key="brightness",
+                y_label_name="Average Brightness",
+                title=self.title,
+                x_ticks_rotation=None,
+                orient="v",
+                show_values=False,
+            )
+        else:
+            plot_options = KDEPlotOptions(
+                x_label_key="brightness",
+                x_label_name="Average Brightness of Images",
+                title=self.title,
+                x_lim=(0, 255),
+                x_ticks_rotation=None,
+                labels_key="split",
+                common_norm=False,
+                fill=True,
+                sharey=True,
+            )
 
-        plot_options = KDEPlotOptions(
-            x_label_key="brightness",
-            x_label_name="Average Brightness of Images",
-            title=self.title,
-            x_lim=(0, 255),
-            x_ticks_rotation=None,
-            labels_key="split",
-            common_norm=False,
-            fill=True,
-            sharey=True,
-        )
         json = dict(train=dict(df[df["split"] == "train"]["brightness"].describe()), val=dict(df[df["split"] == "val"]["brightness"].describe()))
 
         feature = Feature(
             data=df,
             plot_options=plot_options,
             json=json,
         )
@@ -66,11 +82,11 @@
     @property
     def title(self) -> str:
         return "Image Brightness Distribution"
 
     @property
     def description(self) -> str:
         return (
-            "This graph shows the distribution of the image brightness of each dataset. \n"
+            "This graph shows the distribution of the brightness levels across all images. \n"
             "This may for instance uncover differences between the training and validation sets, "
             "such as the presence of exclusively daytime images in the training set and nighttime images in the validation set."
         )
```

## data_gradients/feature_extractors/common/image_color_distribution.py

```diff
@@ -94,9 +94,9 @@
         return "Color Distribution"
 
     @property
     def description(self) -> str:
         return (
             "Here's a comparison of RGB or grayscale intensity intensity (0-255) distributions across the entire dataset, assuming RGB channel ordering. \n"
             "It can reveal discrepancies in the image characteristics between the two datasets, as well as potential flaws in the augmentation process. \n"
-            "E.g., a notable difference in the mean value of a specific color between the two datasets may indicate an issue with augmentation."
+            "E.g., a notable difference in the mean value of a specific color between the two datasets may indicate an issue with the augmentation process."
         )
```

## data_gradients/feature_extractors/common/image_resolution.py

```diff
@@ -79,9 +79,9 @@
         return "Image Width and Height Distribution"
 
     @property
     def description(self) -> str:
         return (
             "These histograms depict the distributions of image height and width. "
             "It's important to note that if certain images have been rescaled or padded, the histograms will represent the size after "
-            "the rescaling and padding operations."
+            "these operations."
         )
```

## data_gradients/feature_extractors/common/sample_visualization.py

```diff
@@ -71,9 +71,9 @@
             "This visualization aids in understanding of the composition of the dataset."
         )
 
     @property
     def notice(self) -> str:
         return (
             f"Only {self.n_cols * self.n_rows} random samples are shown.<br/>"
-            f"You can increase the number of classes by changing `n_cols` and `n_rows` in the configuration file."
+            f"You can increase the number of images by changing `n_cols` and `n_rows` in the configuration file."
         )
```

## data_gradients/feature_extractors/common/summary.py

```diff
@@ -10,19 +10,19 @@
 from data_gradients.assets import assets
 from data_gradients.utils.data_classes.data_samples import ImageSample, SegmentationSample, DetectionSample
 
 
 @dataclasses.dataclass
 class BasicStatistics:
 
-    image_count: int = 0
+    num_samples: int = 0
     classes_count: int = 0
     classes_in_use: int = 0
     classes: List[int] = dataclasses.field(default_factory=list)
-    annotation_count: int = 0
+    num_annotations: int = 0
     images_without_annotation: int = 0
     images_resolutions: List[int] = dataclasses.field(default_factory=list)
     annotations_sizes: List[int] = dataclasses.field(default_factory=list)
     annotations_per_image: List[int] = dataclasses.field(default_factory=list)
     med_image_resolution: int = 0
     smallest_annotations: int = 0
     largest_annotations: int = 0
@@ -43,15 +43,15 @@
     def update(self, sample: ImageSample):
 
         basic_stats = self.stats[sample.split]
 
         height, width = sample.image.shape[:2]
         basic_stats.images_resolutions.append([height, width])
 
-        basic_stats.image_count += 1
+        basic_stats.num_samples += 1
 
         if isinstance(sample, SegmentationSample):
             contours = [contour for sublist in sample.contours for contour in sublist]
             basic_stats.annotations_per_image.append(len(contours))
 
             for contour in contours:
                 basic_stats.annotations_sizes.append(contour.area)
@@ -66,44 +66,44 @@
             basic_stats.annotations_per_image.append(len(boxes))
             for box in boxes:
                 basic_stats.annotations_sizes.append((box[2] - box[0]) * (box[3] - box[1]))
 
             basic_stats.classes_count = len(sample.class_names)
 
     def aggregate(self) -> Feature:
-
         for basic_stats in self.stats.values():
-            basic_stats.classes_in_use = len(set(basic_stats.classes))
+            if basic_stats.num_samples > 0:
+                basic_stats.classes_in_use = len(set(basic_stats.classes))
 
-            basic_stats.classes = np.array(basic_stats.classes)
-            basic_stats.annotations_per_image = np.array(basic_stats.annotations_per_image)
-            basic_stats.annotations_sizes = np.array(basic_stats.annotations_sizes)
-
-            basic_stats.annotation_count = int(np.sum(basic_stats.annotations_per_image))
-            basic_stats.images_without_annotation = np.count_nonzero(basic_stats.annotations_per_image == 0)
-
-            basic_stats.images_resolutions = np.array(basic_stats.images_resolutions)
-            basic_stats.smallest_annotations = int(np.min(basic_stats.annotations_sizes))
-            basic_stats.largest_annotations = int(np.max(basic_stats.annotations_sizes))
-            basic_stats.most_annotations = int(np.max(basic_stats.annotations_per_image))
-            basic_stats.least_annotations = int(np.min(basic_stats.annotations_per_image))
-
-            areas = basic_stats.images_resolutions[:, 0] * basic_stats.images_resolutions[:, 1]
-            areas = areas[:, None]
-            index_of_med = np.argsort(areas)[len(areas) // 2]
-            basic_stats.med_image_resolution = self.format_resolution(basic_stats.images_resolutions[index_of_med][0])
-
-            basic_stats.annotations_per_image = f"{basic_stats.annotation_count / basic_stats.image_count:.2f}"
-            basic_stats.image_count = f"{basic_stats.image_count:,}"
-            basic_stats.annotation_count = f"{basic_stats.annotation_count:,}"
-
-            # To support JSON - delete arrays
-            basic_stats.classes = None
-            basic_stats.images_resolutions = None
-            basic_stats.annotations_sizes = None
+                basic_stats.classes = np.array(basic_stats.classes)
+                basic_stats.annotations_per_image = np.array(basic_stats.annotations_per_image)
+                basic_stats.annotations_sizes = np.array(basic_stats.annotations_sizes)
+
+                basic_stats.num_annotations = int(np.sum(basic_stats.annotations_per_image))
+                basic_stats.images_without_annotation = np.count_nonzero(basic_stats.annotations_per_image == 0)
+
+                basic_stats.images_resolutions = np.array(basic_stats.images_resolutions)
+                basic_stats.smallest_annotations = int(np.min(basic_stats.annotations_sizes))
+                basic_stats.largest_annotations = int(np.max(basic_stats.annotations_sizes))
+                basic_stats.most_annotations = int(np.max(basic_stats.annotations_per_image))
+                basic_stats.least_annotations = int(np.min(basic_stats.annotations_per_image))
+
+                areas = basic_stats.images_resolutions[:, 0] * basic_stats.images_resolutions[:, 1]
+                areas = areas[:, None]
+                index_of_med = np.argsort(areas)[len(areas) // 2]
+                basic_stats.med_image_resolution = self.format_resolution(basic_stats.images_resolutions[index_of_med][0])
+
+                basic_stats.annotations_per_image = f"{basic_stats.num_annotations / basic_stats.num_samples:.2f}"
+                basic_stats.num_samples = int(basic_stats.num_samples)
+                basic_stats.num_annotations = int(basic_stats.num_annotations)
+
+                # To support JSON - delete arrays
+                basic_stats.classes = None
+                basic_stats.images_resolutions = None
+                basic_stats.annotations_sizes = None
 
         json_res = {k: dataclasses.asdict(v) for k, v in self.stats.items()}
 
         feature = Feature(
             data=None,
             plot_options=None,
             json=json_res,
@@ -112,12 +112,12 @@
 
     @property
     def title(self) -> str:
         return "General Statistics"
 
     @property
     def description(self) -> str:
-        return self.template.render(train=self.stats["train"], val=self.stats["val"])
+        return self.template.render(**self.stats)
 
     @staticmethod
     def format_resolution(array: np.ndarray) -> str:
         return "x".join([str(int(x)) for x in array])
```

## data_gradients/feature_extractors/object_detection/bounding_boxes_area.py

```diff
@@ -1,21 +1,32 @@
 import pandas as pd
 
 from data_gradients.common.registry.registry import register_feature_extractor
 from data_gradients.feature_extractors.abstract_feature_extractor import Feature
 from data_gradients.utils.data_classes import DetectionSample
 from data_gradients.visualize.seaborn_renderer import ViolinPlotOptions
 from data_gradients.feature_extractors.abstract_feature_extractor import AbstractFeatureExtractor
+from data_gradients.feature_extractors.utils import MostImportantValuesSelector
 
 
 @register_feature_extractor()
 class DetectionBoundingBoxArea(AbstractFeatureExtractor):
     """Feature Extractor to compute the area covered Bounding Boxes."""
 
-    def __init__(self):
+    def __init__(self, topk: int = 30, prioritization_mode: str = "train_val_diff"):
+        """
+        :param topk:                How many rows (per split) to show.
+        :param prioritization_mode: Strategy to use to chose which class will be prioritized. Only the topk will be shown
+                - 'train_val_diff': Returns the top k rows with the biggest train_val_diff between 'train' and 'val' split values.
+                - 'outliers':       Returns the top k rows with the most extreme average values.
+                - 'max':            Returns the top k rows with the highest average values.
+                - 'min':            Returns the top k rows with the lowest average values.
+                - 'min_max':        Returns the (top k)/2 rows with the biggest average values, and the (top k)/2 with the smallest average values.
+        """
+        self.value_extractor = MostImportantValuesSelector(topk=topk, prioritization_mode=prioritization_mode)
         self.data = []
 
     def update(self, sample: DetectionSample):
         image_area = sample.image.shape[0] * sample.image.shape[1]
         for class_id, bbox_xyxy in zip(sample.class_ids, sample.bboxes_xyxy):
             class_name = sample.class_names[class_id]
             bbox_area = (bbox_xyxy[2] - bbox_xyxy[0]) * (bbox_xyxy[3] - bbox_xyxy[1])
@@ -27,31 +38,39 @@
                     "relative_bbox_area": 100 * (bbox_area / image_area),
                 }
             )
 
     def aggregate(self) -> Feature:
         df = pd.DataFrame(self.data)
 
+        df = self.value_extractor.select(df=df, id_col="class_id", split_col="split", value_col="relative_bbox_area")
+
+        # Height of the plot is proportional to the number of classes
+        n_unique = len(df["class_name"].unique())
+        figsize_x = 10
+        figsize_y = min(max(6, int(n_unique * 0.3)), 175)
+
         max_area = min(100, df["relative_bbox_area"].max())
+
         plot_options = ViolinPlotOptions(
             x_label_key="relative_bbox_area",
             x_label_name="Bounding Box Area (in % of image)",
             y_label_key="class_name",
             y_label_name="Class",
             order_key="class_id",
             title=self.title,
             x_ticks_rotation=None,
             labels_key="split",
             x_lim=(0, max_area),
+            figsize=(figsize_x, figsize_y),
             bandwidth=0.4,
+            tight_layout=True,
         )
 
-        json = dict(
-            train=dict(df[df["split"] == "train"]["relative_bbox_area"].describe()), val=dict(df[df["split"] == "val"]["relative_bbox_area"].describe())
-        )
+        json = {split: dict(df[df["split"] == split]["relative_bbox_area"].describe()) for split in df["split"].unique()}
 
         feature = Feature(
             data=df,
             plot_options=plot_options,
             json=json,
         )
         return feature
@@ -59,12 +78,12 @@
     @property
     def title(self) -> str:
         return "Distribution of Bounding Box Area"
 
     @property
     def description(self) -> str:
         return (
-            "This graph shows the distribution of bounding box area for each class. "
-            "This can highlight distribution gap in object size between the training and validation splits, which can harm the model performance. \n"
-            "Another thing to keep in mind is that having too many very small objects may indicate that your are down sizing your original image to a "
+            "This graph shows the frequency of each class's appearance in the dataset. "
+            "This can highlight distribution gap in object size between the training and validation splits, which can harm the model's performance. \n"
+            "Another thing to keep in mind is that having too many very small objects may indicate that your are downsizing your original image to a "
             "low resolution that is not appropriate for your objects."
         )
```

## data_gradients/feature_extractors/object_detection/bounding_boxes_iou.py

```diff
@@ -70,15 +70,15 @@
 
         if not self.class_agnostic:
             df = df[df["class_id"] == df["other_class_id"]]
 
         data = {}
         json = {}
 
-        splits = df["split"].unique()
+        splits = sorted(df["split"].unique())
         for split in splits:
             counts = self._compute_cumulative_counts_at_thresholds(df[df["split"] == split], class_names, self.num_bins)
 
             json[split] = counts.tolist()
 
             # Add "All classes" label
             counts = np.concatenate([counts, np.sum(counts, axis=0, keepdims=True)], axis=0)
@@ -91,27 +91,30 @@
         num_classes = len(class_names)
         xticklabels = [f"IoU < {bins[x]:.2f}" for x in range(1, len(bins))]
 
         if not data:
             self._show_plot = False
             return Feature(data=None, plot_options=None, json={})
 
+        # Height of the plot is proportional to the number of classes
+        figsize_x = min(max(10, len(bins)), 25)
+        figsize_y = min(max(6, int(num_classes * 0.3)), 175)
+
         plot_options = HeatmapOptions(
             xticklabels=xticklabels,
             yticklabels=class_names + ["All classes"],
             x_label_name="IoU range",
             y_label_name="Class",
             cbar=True,
             fmt="d",
             cmap="rocket_r",
             annot=True,
             title=self.title,
             square=True,
-            # Height of the plot is proportional to the number of classes
-            figsize=(10, (int(num_classes * 0.3) + 4) * len(splits)),
+            figsize=(figsize_x, figsize_y),
             tight_layout=True,
             x_ticks_rotation=90,
         )
 
         feature = Feature(
             data=data,
             plot_options=plot_options,
@@ -123,20 +126,20 @@
     def title(self) -> str:
         return "Intersection of Bounding Boxes"
 
     @property
     def description(self) -> str:
         description = (
             "The distribution of the box Intersection over Union (IoU) with respect to other boxes in the sample. "
-            "The heatmap shows the percentage of boxes overlap with IoU in range [0..T] for each class. "
+            "The heatmap shows the percentage of boxes that overlap with IoU in range [0..T] for each class. "
         )
         if self.class_agnostic:
-            description += "Intersection of all boxes are considered (Regardless of classes of corresponding bboxes)."
+            description += "Intersection of all boxes is considered (Regardless of classes of corresponding bboxes)."
         else:
-            description += "Only intersection of boxes of same class are considered."
+            description += "Only intersection of boxes of same class is considered."
         return description
 
     @property
     def notice(self) -> Optional[str]:
         if not self._show_plot:
             description = "Nothing to show.<br/>"
             if self.class_agnostic:
```

## data_gradients/feature_extractors/object_detection/bounding_boxes_per_image_count.py

```diff
@@ -12,47 +12,44 @@
 class DetectionBoundingBoxPerImageCount(AbstractFeatureExtractor):
     """Feature Extractor to count the number of Bounding Boxes per Image."""
 
     def __init__(self):
         self.data = []
 
     def update(self, sample: DetectionSample):
-        for _ in sample.class_ids:
-            self.data.append(
-                {
-                    "split": sample.split,
-                    "sample_id": sample.sample_id,
-                }
-            )
+        self.data.append(
+            {
+                "split": sample.split,
+                "sample_id": sample.sample_id,
+                "n_bbox": len(sample.bboxes_xyxy),
+            }
+        )
 
     def aggregate(self) -> Feature:
         df = pd.DataFrame(self.data)
 
-        # Include ("sample_id", "split", "n_components")
-        df_class_count = df.groupby(["sample_id", "split"]).size().reset_index(name="n_components")
-
         plot_options = Hist2DPlotOptions(
-            x_label_key="n_components",
+            x_label_key="n_bbox",
             x_label_name="Number of bounding box per Image",
             title=self.title,
             kde=False,
             labels_key="split",
             individual_plots_key="split",
             stat="percent",
             x_ticks_rotation=None,
             sharey=True,
             labels_palette=LABELS_PALETTE,
         )
 
         json = dict(
-            train=dict(df_class_count[df_class_count["split"] == "train"]["n_components"].describe()),
-            val=dict(df_class_count[df_class_count["split"] == "val"]["n_components"].describe()),
+            train=dict(df[df["split"] == "train"]["n_bbox"].describe()),
+            val=dict(df[df["split"] == "val"]["n_bbox"].describe()),
         )
 
-        feature = Feature(data=df_class_count, plot_options=plot_options, json=json)
+        feature = Feature(data=df, plot_options=plot_options, json=json)
         return feature
 
     @property
     def title(self) -> str:
         return "Distribution of Bounding Box per image"
 
     @property
```

## data_gradients/feature_extractors/object_detection/classes_frequency.py

```diff
@@ -1,21 +1,31 @@
 import pandas as pd
-
 from data_gradients.common.registry.registry import register_feature_extractor
 from data_gradients.feature_extractors.abstract_feature_extractor import Feature
 from data_gradients.utils.data_classes import DetectionSample
 from data_gradients.visualize.seaborn_renderer import BarPlotOptions
 from data_gradients.feature_extractors.abstract_feature_extractor import AbstractFeatureExtractor
+from data_gradients.feature_extractors.utils import MostImportantValuesSelector
 
 
 @register_feature_extractor()
 class DetectionClassFrequency(AbstractFeatureExtractor):
     """Feature Extractor to count the number of instance of each class."""
 
-    def __init__(self):
+    def __init__(self, topk: int = 30, prioritization_mode: str = "train_val_diff"):
+        """
+        :param topk:                How many rows (per split) to show.
+        :param prioritization_mode: Strategy to use to chose which class will be prioritized. Only the topk will be shown
+                - 'train_val_diff': Returns the top k rows with the biggest train_val_diff between 'train' and 'val' split values.
+                - 'outliers':       Returns the top k rows with the most extreme average values.
+                - 'max':            Returns the top k rows with the highest average values.
+                - 'min':            Returns the top k rows with the lowest average values.
+                - 'min_max':        Returns the (top k)/2 rows with the biggest average values, and the (top k)/2 with the smallest average values.
+        """
+        self.value_extractor = MostImportantValuesSelector(topk=topk, prioritization_mode=prioritization_mode)
         self.data = []
 
     def update(self, sample: DetectionSample):
         for class_id, bbox_xyxy in zip(sample.class_ids, sample.bboxes_xyxy):
             class_name = sample.class_names[class_id]
             self.data.append(
                 {
@@ -30,30 +40,36 @@
 
         # Include ("class_name", "split", "n_appearance")
         df_class_count = df.groupby(["class_name", "class_id", "split"]).size().reset_index(name="n_appearance")
 
         split_sums = df_class_count.groupby("split")["n_appearance"].sum()
         df_class_count["frequency"] = 100 * (df_class_count["n_appearance"] / df_class_count["split"].map(split_sums))
 
+        df_class_count = self.value_extractor.select(df=df_class_count, id_col="class_id", split_col="split", value_col="frequency")
+
+        # Height of the plot is proportional to the number of classes
+        n_unique = len(df_class_count["class_name"].unique())
+        figsize_x = 10
+        figsize_y = min(max(6, int(n_unique * 0.3)), 175)
+
         plot_options = BarPlotOptions(
             x_label_key="frequency",
             x_label_name="Frequency",
             y_label_key="class_name",
             y_label_name="Class",
             order_key="class_id",
             title=self.title,
+            figsize=(figsize_x, figsize_y),
             x_ticks_rotation=None,
             labels_key="split",
             orient="h",
+            tight_layout=True,
         )
 
-        json = dict(
-            train=dict(df_class_count[df_class_count["split"] == "train"]["n_appearance"].describe()),
-            val=dict(df_class_count[df_class_count["split"] == "val"]["n_appearance"].describe()),
-        )
+        json = {split: dict(df_class_count[df_class_count["split"] == split]["n_appearance"].describe()) for split in df_class_count["split"].unique()}
 
         feature = Feature(
             data=df_class_count,
             plot_options=plot_options,
             json=json,
         )
         return feature
```

## data_gradients/feature_extractors/object_detection/classes_frequency_per_image.py

```diff
@@ -1,22 +1,33 @@
 import pandas as pd
 
 from data_gradients.common.registry.registry import register_feature_extractor
 from data_gradients.feature_extractors.abstract_feature_extractor import Feature
 from data_gradients.utils.data_classes import DetectionSample
 from data_gradients.visualize.plot_options import ViolinPlotOptions
 from data_gradients.feature_extractors.abstract_feature_extractor import AbstractFeatureExtractor
+from data_gradients.feature_extractors.utils import MostImportantValuesSelector
 
 
 @register_feature_extractor()
 class DetectionClassesPerImageCount(AbstractFeatureExtractor):
     """Feature Extractor to show the distribution of number of instance of each class per image.
     This gives information like "The class 'Human' usually appears 2 to 20 times per image."""
 
-    def __init__(self):
+    def __init__(self, topk: int = 30, prioritization_mode: str = "train_val_diff"):
+        """
+        :param topk:                How many rows (per split) to show.
+        :param prioritization_mode: Strategy to use to chose which class will be prioritized. Only the topk will be shown
+                - 'train_val_diff': Returns the top k rows with the biggest train_val_diff between 'train' and 'val' split values.
+                - 'outliers':       Returns the top k rows with the most extreme average values.
+                - 'max':            Returns the top k rows with the highest average values.
+                - 'min':            Returns the top k rows with the lowest average values.
+                - 'min_max':        Returns the (top k)/2 rows with the biggest average values, and the (top k)/2 with the smallest average values.
+        """
+        self.value_extractor = MostImportantValuesSelector(topk=topk, prioritization_mode=prioritization_mode)
         self.data = []
 
     def update(self, sample: DetectionSample):
         for class_id, bbox_xyxy in zip(sample.class_ids, sample.bboxes_xyxy):
             class_name = sample.class_names[class_id]
             self.data.append(
                 {
@@ -29,33 +40,40 @@
 
     def aggregate(self) -> Feature:
         df = pd.DataFrame(self.data)
 
         # Include ("class_name", "class_id", "split", "n_appearance")
         # For each class, image, split, I want to know how many bbox I have
         # TODO: check this
+
         df_class_count = df.groupby(["class_name", "class_id", "sample_id", "split"]).size().reset_index(name="n_appearance")
 
+        df_class_count = self.value_extractor.select(df=df_class_count, id_col="class_id", split_col="split", value_col="n_appearance")
+
+        # Height of the plot is proportional to the number of classes
+        n_unique = len(df_class_count["class_name"].unique())
+        figsize_x = 10
+        figsize_y = min(max(6, int(n_unique * 0.3)), 175)
+
         plot_options = ViolinPlotOptions(
             x_label_key="n_appearance",
             x_label_name="Number of class instance per Image",
             y_label_key="class_name",
             y_label_name="Class Names",
             order_key="class_id",
             title=self.title,
             x_lim=(0, df_class_count["n_appearance"].max() * 1.2),
             bandwidth=0.4,
+            figsize=(figsize_x, figsize_y),
             x_ticks_rotation=None,
             labels_key="split",
+            tight_layout=True,
         )
 
-        json = dict(
-            train=dict(df_class_count[df_class_count["split"] == "train"]["n_appearance"].describe()),
-            val=dict(df_class_count[df_class_count["split"] == "val"]["n_appearance"].describe()),
-        )
+        json = {split: dict(df_class_count[df_class_count["split"] == split]["n_appearance"].describe()) for split in df_class_count["split"].unique()}
 
         feature = Feature(
             data=df_class_count,
             plot_options=plot_options,
             json=json,
         )
         return feature
@@ -63,10 +81,11 @@
     @property
     def title(self) -> str:
         return "Distribution of Class Frequency per Image"
 
     @property
     def description(self) -> str:
         return (
-            "This graph shows how many times each class appears in an image. It highlights whether each class has a constant number of "
-            "appearance per image, or whether it really depends from an image to another."
+            "This graph shows how many times each class appears in an image. "
+            "It highlights whether each class has a constant number of appearances per image, "
+            "or whether there is variability in the number of appearances from image to image."
         )
```

## data_gradients/feature_extractors/object_detection/classes_heatmap_per_class.py

```diff
@@ -30,24 +30,27 @@
             x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
             split_heatmap[class_id, y1:y2, x1:x2] += 1
 
         self.heatmaps_per_split[sample.split] = split_heatmap
 
     @property
     def title(self) -> str:
-        return "Bounding Boxes Density"
+        return "Bounding Box Density"
 
     @property
     def description(self) -> str:
         return (
             "The heatmap represents areas of high object density within the images, providing insights into the spatial distribution of objects. "
-            "By examining the heatmap, you can quickly identify if objects are predominantly concentrated in specific regions or if they are evenly "
+            "By examining the heatmap, you can quickly detect whether objects are predominantly concentrated in specific regions or if they are evenly "
             "distributed throughout the scene. This information can serve as a heuristic to assess if the objects are positioned appropriately "
             "within the expected areas of interest."
         )
 
     @property
     def notice(self) -> str:
-        return (
-            f"Only the {self.n_cols * self.n_rows} classes with highest density are shown.<br/>"
-            f"You can increase the number of classes by changing `n_cols` and `n_rows` in the configuration file."
-        )
+        if len(self.class_names) > self.n_cols * self.n_rows:
+            return (
+                f"Only the {self.n_cols * self.n_rows} classes with highest density are shown.<br/>"
+                f"You can increase the number of classes by changing `n_cols` and `n_rows` in the configuration file."
+            )
+        else:
+            return None
```

## data_gradients/feature_extractors/object_detection/sample_visualization.py

```diff
@@ -1,14 +1,14 @@
 import cv2
 import numpy as np
 
 from data_gradients.common.registry.registry import register_feature_extractor
 from data_gradients.feature_extractors.common.sample_visualization import AbstractSampleVisualization
 from data_gradients.utils.data_classes.data_samples import DetectionSample, ImageChannelFormat
-from data_gradients.visualize.detection import draw_bboxes
+from data_gradients.visualize.detection.detection import draw_bboxes
 
 
 @register_feature_extractor()
 class DetectionSampleVisualization(AbstractSampleVisualization):
     def __init__(self, n_rows: int = 4, n_cols: int = 2, stack_splits_vertically: bool = True):
         """
         :param n_rows:     Number of rows to use per split
```

## data_gradients/feature_extractors/segmentation/bounding_boxes_area.py

```diff
@@ -1,58 +1,78 @@
 import pandas as pd
 
 from data_gradients.common.registry.registry import register_feature_extractor
 from data_gradients.feature_extractors.abstract_feature_extractor import Feature
 from data_gradients.utils.data_classes import SegmentationSample
 from data_gradients.visualize.seaborn_renderer import ViolinPlotOptions
 from data_gradients.feature_extractors.abstract_feature_extractor import AbstractFeatureExtractor
+from data_gradients.feature_extractors.utils import MostImportantValuesSelector
 
 
 @register_feature_extractor()
 class SegmentationBoundingBoxArea(AbstractFeatureExtractor):
     """
     Semantic Segmentation task feature extractor -
     Get all Bounding Boxes areas and plot them as a percentage of the whole image.
     """
 
-    def __init__(self):
+    def __init__(self, topk: int = 30, prioritization_mode: str = "train_val_diff"):
+        """
+        :param topk:                How many rows (per split) to show.
+        :param prioritization_mode: Strategy to use to chose which class will be prioritized. Only the topk will be shown
+                - 'train_val_diff': Returns the top k rows with the biggest train_val_diff between 'train' and 'val' split values.
+                - 'outliers':       Returns the top k rows with the most extreme average values.
+                - 'max':            Returns the top k rows with the highest average values.
+                - 'min':            Returns the top k rows with the lowest average values.
+                - 'min_max':        Returns the (top k)/2 rows with the biggest average values, and the (top k)/2 with the smallest average values.
+        """
+        self.value_extractor = MostImportantValuesSelector(topk=topk, prioritization_mode=prioritization_mode)
         self.data = []
 
     def update(self, sample: SegmentationSample):
         image_area = sample.image.shape[0] * sample.image.shape[1]
         for class_channel in sample.contours:
             for contour in class_channel:
                 class_id = contour.class_id
                 class_name = sample.class_names[class_id]
                 self.data.append(
                     {
                         "split": sample.split,
                         "class_name": class_name,
                         "class_id": class_id,
-                        "bbox_area": 100 * (contour.bbox_area / image_area),
+                        "relative_bbox_area": 100 * (contour.bbox_area / image_area),
                     }
                 )
 
     def aggregate(self) -> Feature:
         df = pd.DataFrame(self.data)
 
-        max_area = min(100, df["bbox_area"].max())
+        df = self.value_extractor.select(df=df, id_col="class_id", split_col="split", value_col="relative_bbox_area")
+
+        # Height of the plot is proportional to the number of classes
+        n_unique = len(df["class_name"].unique())
+        figsize_x = 10
+        figsize_y = min(max(6, int(n_unique * 0.3)), 175)
+
+        max_area = min(100, df["relative_bbox_area"].max())
         plot_options = ViolinPlotOptions(
-            x_label_key="bbox_area",
-            x_label_name="Bounding Box Area (in % of image)",
+            x_label_key="relative_bbox_area",
+            x_label_name="Object Area (in % of image)",
             y_label_key="class_name",
             y_label_name="Class",
             order_key="class_id",
             title=self.title,
+            figsize=(figsize_x, figsize_y),
             x_lim=(0, max_area),
             x_ticks_rotation=None,
             labels_key="split",
             bandwidth=0.4,
+            tight_layout=True,
         )
-        json = dict(train=dict(df[df["split"] == "train"]["bbox_area"].describe()), val=dict(df[df["split"] == "val"]["bbox_area"].describe()))
+        json = {split: dict(df[df["split"] == split]["relative_bbox_area"].describe()) for split in df["split"].unique()}
 
         feature = Feature(
             data=df,
             plot_options=plot_options,
             json=json,
         )
         return feature
@@ -60,12 +80,12 @@
     @property
     def title(self) -> str:
         return "Distribution of Object Area"
 
     @property
     def description(self) -> str:
         return (
-            "This graph shows the distribution of object area for each class. "
-            "This can highlight distribution gap in object size between the training and validation splits, which can harm the model performance. \n"
-            "Another thing to keep in mind is that having too many very small objects may indicate that your are down sizing your original image to a "
+            "This graph shows the frequency of each class's appearance in the dataset. "
+            "This can highlight distribution gap in object size between the training and validation splits, which can harm the model's performance. \n"
+            "Another thing to keep in mind is that having too many very small objects may indicate that your are downsizing your original image to a "
             "low resolution that is not appropriate for your objects."
         )
```

## data_gradients/feature_extractors/segmentation/classes_frequency.py

```diff
@@ -1,19 +1,30 @@
 import pandas as pd
 
 from data_gradients.common.registry.registry import register_feature_extractor
 from data_gradients.feature_extractors.abstract_feature_extractor import Feature
 from data_gradients.utils.data_classes import SegmentationSample
 from data_gradients.visualize.seaborn_renderer import BarPlotOptions
 from data_gradients.feature_extractors.abstract_feature_extractor import AbstractFeatureExtractor
+from data_gradients.feature_extractors.utils import MostImportantValuesSelector
 
 
 @register_feature_extractor()
 class SegmentationClassFrequency(AbstractFeatureExtractor):
-    def __init__(self):
+    def __init__(self, topk: int = 30, prioritization_mode: str = "train_val_diff"):
+        """
+        :param topk:                How many rows (per split) to show.
+        :param prioritization_mode: Strategy to use to chose which class will be prioritized. Only the topk will be shown
+                - 'train_val_diff': Returns the top k rows with the biggest train_val_diff between 'train' and 'val' split values.
+                - 'outliers':       Returns the top k rows with the most extreme average values.
+                - 'max':            Returns the top k rows with the highest average values.
+                - 'min':            Returns the top k rows with the lowest average values.
+                - 'min_max':        Returns the (top k)/2 rows with the biggest average values, and the (top k)/2 with the smallest average values.
+        """
+        self.value_extractor = MostImportantValuesSelector(topk=topk, prioritization_mode=prioritization_mode)
         self.data = []
 
     def update(self, sample: SegmentationSample):
         for j, class_channel in enumerate(sample.contours):
             for contour in class_channel:
                 class_id = contour.class_id
                 class_name = sample.class_names[class_id]
@@ -30,30 +41,36 @@
 
         # Include ("class_name", "class_id", "split", "n_appearance")
         df_class_count = df.groupby(["class_name", "class_id", "split"]).size().reset_index(name="n_appearance")
 
         split_sums = df_class_count.groupby("split")["n_appearance"].sum()
         df_class_count["frequency"] = 100 * (df_class_count["n_appearance"] / df_class_count["split"].map(split_sums))
 
+        df_class_count = self.value_extractor.select(df=df_class_count, id_col="class_id", split_col="split", value_col="frequency")
+
+        # Height of the plot is proportional to the number of classes
+        n_unique = len(df_class_count["class_name"].unique())
+        figsize_x = 10
+        figsize_y = min(max(6, int(n_unique * 0.3)), 175)
+
         plot_options = BarPlotOptions(
             x_label_key="frequency",
             x_label_name="Frequency",
             y_label_key="class_name",
             y_label_name="Class",
             order_key="class_id",
             title=self.title,
+            figsize=(figsize_x, figsize_y),
             x_ticks_rotation=None,
             labels_key="split",
             orient="h",
+            tight_layout=True,
         )
 
-        json = dict(
-            train=dict(df_class_count[df_class_count["split"] == "train"]["n_appearance"].describe()),
-            val=dict(df_class_count[df_class_count["split"] == "val"]["n_appearance"].describe()),
-        )
+        json = {split: dict(df_class_count[df_class_count["split"] == split]["n_appearance"].describe()) for split in df_class_count["split"].unique()}
 
         feature = Feature(
             data=df_class_count,
             plot_options=plot_options,
             json=json,
         )
         return feature
```

## data_gradients/feature_extractors/segmentation/classes_frequency_per_image.py

```diff
@@ -1,19 +1,30 @@
 import pandas as pd
 
 from data_gradients.common.registry.registry import register_feature_extractor
 from data_gradients.feature_extractors.abstract_feature_extractor import Feature
 from data_gradients.utils.data_classes import SegmentationSample
 from data_gradients.visualize.plot_options import ViolinPlotOptions
 from data_gradients.feature_extractors.abstract_feature_extractor import AbstractFeatureExtractor
+from data_gradients.feature_extractors.utils import MostImportantValuesSelector
 
 
 @register_feature_extractor()
 class SegmentationClassesPerImageCount(AbstractFeatureExtractor):
-    def __init__(self):
+    def __init__(self, topk: int = 30, prioritization_mode: str = "train_val_diff"):
+        """
+        :param topk:                How many rows (per split) to show.
+        :param prioritization_mode: Strategy to use to chose which class will be prioritized. Only the topk will be shown
+                - 'train_val_diff': Returns the top k rows with the biggest train_val_diff between 'train' and 'val' split values.
+                - 'outliers':       Returns the top k rows with the most extreme average values.
+                - 'max':            Returns the top k rows with the highest average values.
+                - 'min':            Returns the top k rows with the lowest average values.
+                - 'min_max':        Returns the (top k)/2 rows with the biggest average values, and the (top k)/2 with the smallest average values.
+        """
+        self.value_extractor = MostImportantValuesSelector(topk=topk, prioritization_mode=prioritization_mode)
         self.data = []
 
     def update(self, sample: SegmentationSample):
 
         for j, class_channel in enumerate(sample.contours):
             for contour in class_channel:
                 class_id = contour.class_id
@@ -30,33 +41,39 @@
     def aggregate(self) -> Feature:
         df = pd.DataFrame(self.data)
 
         # Include ("class_name", "class_id", "split", "n_appearance")
         # For each class, image, split, I want to know how many bbox I have
         df_class_count = df.groupby(["class_name", "class_id", "sample_id", "split"]).size().reset_index(name="n_appearance")
 
+        df_class_count = self.value_extractor.select(df=df_class_count, id_col="class_id", split_col="split", value_col="n_appearance")
+
         max_n_appearance = df_class_count["n_appearance"].max()
 
+        # Height of the plot is proportional to the number of classes
+        n_unique = len(df_class_count["class_name"].unique())
+        figsize_x = 10
+        figsize_y = min(max(6, int(n_unique * 0.3)), 175)
+
         plot_options = ViolinPlotOptions(
             x_label_key="n_appearance",
             x_label_name="Number of class instance per Image",
             y_label_key="class_name",
             y_label_name="Class Names",
             order_key="class_id",
             title=self.title,
             x_lim=(0, max_n_appearance * 1.2),  # Cut the max_x at 120% of the highest max n_appearance to increase readability
+            figsize=(figsize_x, figsize_y),
             bandwidth=0.4,
             x_ticks_rotation=None,
             labels_key="split",
+            tight_layout=True,
         )
 
-        json = dict(
-            train=dict(df_class_count[df_class_count["split"] == "train"]["n_appearance"].describe()),
-            val=dict(df_class_count[df_class_count["split"] == "val"]["n_appearance"].describe()),
-        )
+        json = {split: dict(df_class_count[df_class_count["split"] == split]["n_appearance"].describe()) for split in df_class_count["split"].unique()}
 
         feature = Feature(
             data=df_class_count,
             plot_options=plot_options,
             json=json,
         )
         return feature
@@ -64,10 +81,11 @@
     @property
     def title(self) -> str:
         return "Distribution of Class Frequency per Image"
 
     @property
     def description(self) -> str:
         return (
-            "This graph shows how many times each class appears in an image. It highlights whether each class has a constant number of "
-            "appearance per image, or whether it really depends from an image to another."
+            "This graph shows how many times each class appears in an image. "
+            "It highlights whether each class has a constant number of appearances per image, "
+            "or whether there is variability in the number of appearances from image to image."
         )
```

## data_gradients/feature_extractors/segmentation/classes_heatmap_per_class.py

```diff
@@ -37,18 +37,21 @@
     def title(self) -> str:
         return "Objects Density"
 
     @property
     def description(self) -> str:
         return (
             "The heatmap represents areas of high object density within the images, providing insights into the spatial distribution of objects. "
-            "By examining the heatmap, you can quickly identify if objects are predominantly concentrated in specific regions or if they are evenly "
+            "By examining the heatmap, you can quickly detect whether objects are predominantly concentrated in specific regions or if they are evenly "
             "distributed throughout the scene. This information can serve as a heuristic to assess if the objects are positioned appropriately "
             "within the expected areas of interest."
         )
 
     @property
     def notice(self) -> str:
-        return (
-            f"Only the {self.n_cols * self.n_rows} classes with highest density are shown.<br/>"
-            f"You can increase the number of classes by changing `n_cols` and `n_rows` in the configuration file."
-        )
+        if len(self.class_names) > self.n_cols * self.n_rows:
+            return (
+                f"Only the {self.n_cols * self.n_rows} classes with highest density are shown.<br/>"
+                f"You can increase the number of classes by changing `n_cols` and `n_rows` in the configuration file."
+            )
+        else:
+            return None
```

## data_gradients/feature_extractors/segmentation/component_frequency_per_image.py

```diff
@@ -11,46 +11,43 @@
 @register_feature_extractor()
 class SegmentationComponentsPerImageCount(AbstractFeatureExtractor):
     def __init__(self):
         self.data = []
 
     def update(self, sample: SegmentationSample):
         for j, class_channel in enumerate(sample.contours):
-            for contour in class_channel:
-                self.data.append(
-                    {
-                        "split": sample.split,
-                        "sample_id": sample.sample_id,
-                    }
-                )
+            self.data.append(
+                {
+                    "split": sample.split,
+                    "sample_id": sample.sample_id,
+                    "n_components": len(class_channel),
+                }
+            )
 
     def aggregate(self) -> Feature:
         df = pd.DataFrame(self.data)
 
-        # Include ("sample_id", "split", "n_components")
-        df_class_count = df.groupby(["sample_id", "split"]).size().reset_index(name="n_components")
-
         plot_options = Hist2DPlotOptions(
             x_label_key="n_components",
             x_label_name="Number of component per Image",
             title=self.title,
             kde=False,
             labels_key="split",
             individual_plots_key="split",
             x_ticks_rotation=None,
             sharey=True,
             labels_palette=LABELS_PALETTE,
         )
 
         json = dict(
-            train=dict(df_class_count[df_class_count["split"] == "train"]["n_components"].describe()),
-            val=dict(df_class_count[df_class_count["split"] == "val"]["n_components"].describe()),
+            train=dict(df[df["split"] == "train"]["n_components"].describe()),
+            val=dict(df[df["split"] == "val"]["n_components"].describe()),
         )
 
-        feature = Feature(data=df_class_count, plot_options=plot_options, json=json)
+        feature = Feature(data=df, plot_options=plot_options, json=json)
         return feature
 
     @property
     def title(self) -> str:
         return "Distribution of Objects per Image"
 
     @property
```

## data_gradients/feature_extractors/segmentation/components_convexity.py

```diff
@@ -47,15 +47,15 @@
             plot_options=plot_options,
             json=json,
         )
         return feature
 
     @property
     def title(self) -> str:
-        return "Objects Convexity"
+        return "Object Convexity"
 
     @property
     def description(self) -> str:
         return (
             "This graph depicts the convexity distribution of objects in both training and validation sets. \n"
             "Higher convexity values suggest complex structures that may pose challenges for accurate segmentation."
         )
```

## data_gradients/feature_extractors/segmentation/components_erosion.py

```diff
@@ -56,15 +56,15 @@
             val=dict(df[df["split"] == "val"]["percent_change_of_n_components"].describe()),
         )
 
         return Feature(data=df, plot_options=plot_options, json=json)
 
     @property
     def title(self) -> str:
-        return "Objects Stability to Erosion"
+        return "Object Stability to Erosion"
 
     @property
     def description(self) -> str:
         return (
             "Assessment of object stability under morphological opening - erosion followed by dilation. "
             "When a lot of components are small then the number of components decrease which means we might have "
             "noise in our annotations (i.e 'sprinkles')."
```

## data_gradients/managers/abstract_manager.py

```diff
@@ -34,25 +34,27 @@
         train_data: Iterable,
         val_data: Optional[Iterable] = None,
         report_subtitle: Optional[str] = None,
         log_dir: Optional[str] = None,
         batch_processor: BatchProcessor,
         grouped_feature_extractors: Dict[str, List[AbstractFeatureExtractor]],
         batches_early_stop: Optional[int] = None,
+        remove_plots_after_report: Optional[bool] = True,
     ):
         """
         :param report_title:        Title of the report. Will be used to save the report
         :param report_subtitle:     Subtitle of the report
         :param train_data:          Iterable object contains images and labels of the training dataset
         :param val_data:            Iterable object contains images and labels of the validation dataset
         :param log_dir:             Directory where to save the logs. By default uses the current working directory
         :param batch_processor:     Batch processor object to be used before extracting features
         :param grouped_feature_extractors:  List of feature extractors to be used
         :param id_to_name:          Dictionary mapping class IDs to class names
         :param batches_early_stop:  Maximum number of batches to run in training (early stop)
+        :param remove_plots_after_report:  Delete the plots from the report directory after the report is generated. By default, True
         """
         self.renderer = SeabornRenderer()
         self.summary_writer = SummaryWriter(report_title=report_title, report_subtitle=report_subtitle, log_dir=log_dir)
 
         self.data_config_cache_name = f"{self.summary_writer.run_name}.json"
         self.data_config = data_config
         self.data_config.fill_missing_params_with_cache(cache_filename=self.data_config_cache_name)
@@ -66,15 +68,18 @@
 
         self.train_iter = iter(train_data)
         self.val_iter = iter(val_data) if val_data is not None else iter([])
 
         # FEATURES
         self.batch_processor = batch_processor
         self.grouped_feature_extractors = grouped_feature_extractors
-
+        self._remove_plots_after_report = remove_plots_after_report
+        for _, grouped_feature_list in self.grouped_feature_extractors.items():
+            for feature_extractor in grouped_feature_list:
+                feature_extractor.setup_data_sources(train_data, val_data)
         self._train_iters_done = 0
         self._val_iters_done = 0
         self._train_batch_size = None
         self._val_batch_size = None
         self._stopped_early = None
 
     def execute(self):
@@ -152,15 +157,15 @@
                     feature_json = {"error": error_description}
                     feature_error = f"Feature extraction error. Check out the log file for more details:<br/>" f"<em>{self.summary_writer.errors_path}</em>"
                     self.summary_writer.add_error(title=feature_extractor.title, error=error_description)
 
                 if f is not None:
                     image_name = feature_extractor.__class__.__name__ + ".png"
                     image_path = os.path.join(self.summary_writer.archive_dir, image_name)
-                    f.savefig(image_path, dpi=300)
+                    f.savefig(image_path, dpi=200)
                     images_created.append(image_path)
                 else:
                     image_path = None
 
                 self.summary_writer.add_feature_stats(title=feature_extractor.title, stats=feature_json)
 
                 if feature_error:
@@ -169,15 +174,15 @@
                     warning = self._create_samples_iterated_warning()
                 else:
                     warning = feature_extractor.warning
 
                 section.add_feature(
                     FeatureSummary(
                         name=feature_extractor.title,
-                        description=feature_extractor.description,
+                        description=self._format_feature_description(feature_extractor.description),
                         image_path=image_path,
                         warning=warning,
                         notice=feature_extractor.notice,
                     )
                 )
             summary.add_section(section)
 
@@ -187,16 +192,17 @@
         self.summary_writer.set_data_config(data_config_dict=self.data_config.to_json())
         self.summary_writer.write()
 
         # Save cache in a specific Folder
         self.data_config.write_to_json(filename=self.data_config_cache_name)
 
         # Cleanup of generated images
-        for image_created in images_created:
-            os.remove(image_created)
+        if self._remove_plots_after_report:
+            for image_created in images_created:
+                os.remove(image_created)
 
     def close(self):
         """Safe logging closing"""
         print(f'{"*" * 100}')
         print("We have finished evaluating your dataset!")
         print()
         print("The cache of your DataConfig object can be found in:")
@@ -249,7 +255,14 @@
             total_val_samples = self.val_size * self._val_batch_size
             portion_val = f" ({self._val_iters_done/total_val_samples:.1%})"
 
         msg_head = "The results presented in this report cover only a subset of the data.\n"
         msg_train = f"Train set: {self._train_iters_done} out of {total_train_samples} samples were analyzed{portion_train}.\n"
         msg_val = f"Validation set: {self._val_iters_done} out of {total_val_samples} samples were analyzed{portion_val}.\n "
         return msg_head + msg_train + msg_val
+
+    @staticmethod
+    def _format_feature_description(description: str) -> str:
+        """
+        Formats the feature extractor's description string for a vieable display in HTML.
+        """
+        return description.replace("\n", "<br />")
```

## data_gradients/managers/detection_manager.py

```diff
@@ -1,16 +1,16 @@
-import os
 from typing import Optional, Iterable, Callable, List
+
 import torch
 
-from data_gradients.managers.abstract_manager import AnalysisManagerAbstract
-from data_gradients.config.utils import load_report_feature_extractors
 from data_gradients.batch_processors.detection import DetectionBatchProcessor
 from data_gradients.config.data.data_config import DetectionDataConfig
-from data_gradients.config.data.typing import SupportedDataType
+from data_gradients.config.data.typing import SupportedDataType, FeatureExtractorsType
+from data_gradients.config.utils import get_grouped_feature_extractors
+from data_gradients.managers.abstract_manager import AnalysisManagerAbstract
 
 
 class DetectionAnalysisManager(AnalysisManagerAbstract):
     """Main detection manager class.
     Definition of task name, task-related preprocessor and parsing related configuration file
     """
 
@@ -18,46 +18,55 @@
         self,
         *,
         report_title: str,
         train_data: Iterable,
         val_data: Optional[Iterable] = None,
         report_subtitle: Optional[str] = None,
         config_path: Optional[str] = None,
+        feature_extractors: Optional[FeatureExtractorsType] = None,
         log_dir: Optional[str] = None,
         use_cache: bool = False,
         class_names: Optional[List[str]] = None,
         class_names_to_use: Optional[List[str]] = None,
         n_classes: Optional[int] = None,
         images_extractor: Optional[Callable[[SupportedDataType], torch.Tensor]] = None,
         labels_extractor: Optional[Callable[[SupportedDataType], torch.Tensor]] = None,
         is_label_first: Optional[bool] = None,
         bbox_format: Optional[str] = None,
         n_image_channels: int = 3,
         batches_early_stop: int = 999,
+        remove_plots_after_report: Optional[bool] = True,
     ):
         """
         Constructor of detection manager which controls the analyzer
         :param report_title:            Title of the report. Will be used to save the report
         :param report_subtitle:         Subtitle of the report
         :param class_names:             List of all class names in the dataset. The index should represent the class_id.
         :param class_names_to_use:      List of class names that we should use for analysis.
         :param n_classes:               Number of classes. Mutually exclusive with `class_names`.
         :param train_data:              Iterable object contains images and labels of the training dataset
         :param val_data:                Iterable object contains images and labels of the validation dataset
-        :param config_path:             Full path the hydra configuration file. If None, the default configuration will be used.
+        :param config_path:             Full path the hydra configuration file. If None, the default configuration will be used. Mutually exclusive
+                                        with feature_extractors
+        :param feature_extractors:      One or more feature extractors to use. If None, the default configuration will be used. Mutually exclusive
+                                        with config_path
         :param log_dir:                 Directory where to save the logs. By default uses the current working directory
         :param batches_early_stop:      Maximum number of batches to run in training (early stop)
         :param use_cache:               Whether to use cache or not for the configuration of the data.
         :param images_extractor:        Function extracting the image(s) out of the data output.
         :param labels_extractor:        Function extracting the label(s) out of the data output.
         :param is_label_first:          Whether the labels are in the first dimension or not.
                                             > (class_id, x, y, w, h) for instance, as opposed to (x, y, w, h, class_id)
         :param bbox_format:             Format of the bounding boxes. 'xyxy', 'xywh' or 'cxcywh'
         :param n_image_channels:        Number of channels for each image in the dataset
+        :param remove_plots_after_report:  Delete the plots from the report directory after the report is generated. By default, True
         """
+        if feature_extractors is not None and config_path is not None:
+            raise RuntimeError("`feature_extractors` and `config_path` cannot be specified at the same time")
+
         data_config = DetectionDataConfig(
             use_cache=use_cache,
             images_extractor=images_extractor,
             labels_extractor=labels_extractor,
             is_label_first=is_label_first,
             xyxy_converter=bbox_format,
         )
@@ -72,34 +81,32 @@
         # Define `class_names_to_use`
         if class_names_to_use:
             invalid_class_names_to_use = set(class_names_to_use) - set(class_names)
             if invalid_class_names_to_use != set():
                 raise RuntimeError(f"You defined `class_names_to_use` with classes that are not listed in `class_names`: {invalid_class_names_to_use}")
         class_names_to_use = class_names_to_use or class_names
 
-        # Resolve `config_dir` and `config_name` defining the feature extractors.
-        if config_path is None:
-            config_dir, config_name = None, "detection"
-        else:
-            config_path = os.path.abspath(config_path)
-            config_dir, config_name = os.path.dirname(config_path), os.path.basename(config_path).split(".")[0]
+        grouped_feature_extractors = get_grouped_feature_extractors(
+            default_config_name="detection",
+            config_path=config_path,
+            feature_extractors=feature_extractors,
+        )
 
         batch_processor = DetectionBatchProcessor(
             data_config=data_config,
             n_image_channels=n_image_channels,
             class_names=class_names,
             class_names_to_use=class_names_to_use,
         )
 
-        feature_extractors = load_report_feature_extractors(config_name=config_name, config_dir=config_dir)
-
         super().__init__(
             data_config=data_config,
             report_title=report_title,
             report_subtitle=report_subtitle,
             train_data=train_data,
             val_data=val_data,
             batch_processor=batch_processor,
-            grouped_feature_extractors=feature_extractors,
+            grouped_feature_extractors=grouped_feature_extractors,
             log_dir=log_dir,
             batches_early_stop=batches_early_stop,
+            remove_plots_after_report=remove_plots_after_report
         )
```

## data_gradients/managers/segmentation_manager.py

```diff
@@ -1,16 +1,15 @@
-import os
 from typing import Optional, Iterable, Callable, List
 import torch
 
+from data_gradients.config.utils import get_grouped_feature_extractors
 from data_gradients.managers.abstract_manager import AnalysisManagerAbstract
-from data_gradients.config.utils import load_report_feature_extractors
 from data_gradients.batch_processors.segmentation import SegmentationBatchProcessor
 from data_gradients.config.data.data_config import SegmentationDataConfig
-from data_gradients.config.data.typing import SupportedDataType
+from data_gradients.config.data.typing import SupportedDataType, FeatureExtractorsType
 
 
 class SegmentationAnalysisManager(AnalysisManagerAbstract):
     """
     Main semantic segmentation manager class.
     Definition of task name, task-related preprocessor and parsing related configuration file
     """
@@ -19,45 +18,54 @@
         self,
         *,
         report_title: str,
         train_data: Iterable,
         val_data: Optional[Iterable] = None,
         report_subtitle: Optional[str] = None,
         config_path: Optional[str] = None,
+        feature_extractors: Optional[FeatureExtractorsType] = None,
         log_dir: Optional[str] = None,
         use_cache: bool = False,
         class_names: Optional[List[str]] = None,
         class_names_to_use: Optional[List[str]] = None,
         n_classes: Optional[int] = None,
         images_extractor: Optional[Callable[[SupportedDataType], torch.Tensor]] = None,
         labels_extractor: Optional[Callable[[SupportedDataType], torch.Tensor]] = None,
         num_image_channels: int = 3,
         threshold_soft_labels: float = 0.5,
         batches_early_stop: int = 999,
+        remove_plots_after_report: Optional[bool] = True,
     ):
         """
         Constructor of semantic-segmentation manager which controls the analyzer
 
         :param report_title:            Title of the report. Will be used to save the report
         :param report_subtitle:         Subtitle of the report
         :param class_names:             List of all class names in the dataset. The index should represent the class_id. Mutually exclusive with `n_classes`
         :param class_names_to_use:      List of class names that we should use for analysis.
         :param n_classes:               Number of classes. Mutually exclusive with `class_names`. If set, `class_names` will be a list of `class_ids`.
         :param train_data:              Iterable object contains images and labels of the training dataset
         :param val_data:                Iterable object contains images and labels of the validation dataset
-        :param config_path:             Full path the hydra configuration file. If None, the default configuration will be used.
+        :param config_path:             Full path the hydra configuration file. If None, the default configuration will be used. Mutually exclusive
+                                        with feature_extractors
+        :param feature_extractors:      One or more feature extractors to use. If None, the default configuration will be used. Mutually exclusive
+                                        with config_path
         :param log_dir:                 Directory where to save the logs. By default uses the current working directory
         :param id_to_name:              Class ID to class names mapping (Dictionary)
         :param batches_early_stop:      Maximum number of batches to run in training (early stop)
         :param use_cache:               Whether to use cache or not for the configuration of the data.
         :param images_extractor:        Function extracting the image(s) out of the data output.
         :param labels_extractor:        Function extracting the label(s) out of the data output.
         :param num_image_channels:      Number of channels for each image in the dataset
         :param threshold_soft_labels:   Threshold for converting soft labels to binary labels
+        :param remove_plots_after_report:  Delete the plots from the report directory after the report is generated. By default, True
         """
+        if feature_extractors is not None and config_path is not None:
+            raise RuntimeError("`feature_extractors` and `config_path` cannot be specified at the same time")
+
         data_config = SegmentationDataConfig(use_cache=use_cache, images_extractor=images_extractor, labels_extractor=labels_extractor)
 
         # Check values of `n_classes` and `class_names` to define `class_names`.
         if n_classes and class_names:
             raise RuntimeError("`class_names` and `n_classes` cannot be specified at the same time")
         elif n_classes is None and class_names is None:
             raise RuntimeError("Either `class_names` or `n_classes` must be specified")
@@ -66,35 +74,33 @@
         # Define `class_names_to_use`
         if class_names_to_use:
             invalid_class_names_to_use = set(class_names_to_use) - set(class_names)
             if invalid_class_names_to_use != set():
                 raise RuntimeError(f"You defined `class_names_to_use` with classes that are not listed in `class_names`: {invalid_class_names_to_use}")
         class_names_to_use = class_names_to_use or class_names
 
-        # Resolve `config_dir` and `config_name` defining the feature extractors.
-        if config_path is None:
-            config_dir, config_name = None, "segmentation"
-        else:
-            config_path = os.path.abspath(config_path)
-            config_dir, config_name = os.path.dirname(config_path), os.path.basename(config_path).split(".")[0]
+        grouped_feature_extractors = get_grouped_feature_extractors(
+            default_config_name="segmentation",
+            config_path=config_path,
+            feature_extractors=feature_extractors,
+        )
 
         batch_processor = SegmentationBatchProcessor(
             data_config=data_config,
             class_names=class_names,
             class_names_to_use=class_names_to_use,
             n_image_channels=num_image_channels,
             threshold_value=threshold_soft_labels,
         )
 
-        grouped_feature_extractors = load_report_feature_extractors(config_name=config_name, config_dir=config_dir)
-
         super().__init__(
             data_config=data_config,
             report_title=report_title,
             report_subtitle=report_subtitle,
             train_data=train_data,
             val_data=val_data,
             batch_processor=batch_processor,
             grouped_feature_extractors=grouped_feature_extractors,
             log_dir=log_dir,
             batches_early_stop=batches_early_stop,
+            remove_plots_after_report=remove_plots_after_report
         )
```

## data_gradients/utils/detection.py

```diff
@@ -29,17 +29,17 @@
     x2 = x1 + w
     y2 = y1 + h
 
     return torch.stack([x1, y1, x2, y2], dim=-1)
 
 
 XYXY_CONVERTERS = {
-    "xyxy": {"function": lambda x: x, "description": "xyxy: x-left, y-top, x-right, y-bottom"},
-    "xywh": {"function": xywh_to_xyxy, "description": "xywh: x-left, y-top, width, height"},
-    "cxcywh": {"function": cxcywh_to_xyxy, "description": "cxcywh: x-center, y-center, width, height"},
+    "xyxy": {"function": lambda x: x, "description": "xyxy: x-left, y-top, x-right, y-bottom\t\t(Pascal-VOC format)"},
+    "xywh": {"function": xywh_to_xyxy, "description": "xywh: x-left, y-top, width, height\t\t\t(COCO format)"},
+    "cxcywh": {"function": cxcywh_to_xyxy, "description": "cxcywh: x-center, y-center, width, height\t\t(YOLO format)"},
 }
 
 
 class XYXYConvertError(Exception):
     ...
```

## data_gradients/utils/data_classes/data_samples.py

```diff
@@ -8,14 +8,15 @@
 
 
 class ImageChannelFormat(Enum):
     RGB = "RGB"
     BGR = "BGR"
     GRAYSCALE = "GRAYSCALE"
     UNKNOWN = "UNKNOWN"
+    UNCHANGED = "UNCHANGED"
 
 
 @dataclasses.dataclass
 class ImageSample:
     """
     This is a dataclass that represents a single sample of the dataset where input to the model is a single image.
```

## data_gradients/visualize/images.py

```diff
@@ -57,15 +57,15 @@
         plt.tight_layout()
 
     return fig_to_array(fig)
 
 
 def fig_to_array(fig: plt.Figure) -> np.ndarray:
     buf = io.BytesIO()
-    fig.savefig(buf, format="png")
+    fig.savefig(buf, format="png", dpi=200)
     buf.seek(0)
     image = Image.open(buf)
     plt.close(fig)
     return np.asarray(image)
 
 
 def combine_images_per_split_per_class(images_per_split_per_class: Dict[str, Dict[str, np.ndarray]], n_cols: int) -> plt.Figure:
```

## data_gradients/visualize/seaborn_renderer.py

```diff
@@ -61,16 +61,14 @@
             dfs = [df[df[options.individual_plots_key] == key] for key in df[options.individual_plots_key].unique()]
             _num_images = len(dfs)
             _max_cols = options.individual_plots_max_cols
             n_cols = _num_images if _max_cols is None else min(_num_images, _max_cols)
             n_rows = int(np.ceil(_num_images / n_cols))
 
         fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=options.figsize, sharey=options.sharey)
-        if options.tight_layout:
-            fig.tight_layout()
         fig.subplots_adjust(top=0.9)
 
         if n_rows == 1 and n_cols == 1:
             axs = [axs]
         else:
             axs = axs.reshape(-1)
 
@@ -105,14 +103,16 @@
                 if n_unique > 50:
                     options.x_ticks_rotation = 90
                 elif n_unique > 10:
                     options.x_ticks_rotation = 45
 
             self._set_ticks_rotation(ax_i, options.x_ticks_rotation, options.y_ticks_rotation)
 
+        if options.tight_layout:
+            fig.tight_layout()
         return fig
 
     def _render_histplot(self, df, options: Hist2DPlotOptions) -> plt.Figure:
 
         if options.individual_plots_key is None:
             dfs = [df]
             n_rows = 1
@@ -121,16 +121,14 @@
             dfs = [df[df[options.individual_plots_key] == key] for key in df[options.individual_plots_key].unique()]
             _num_images = len(dfs)
             _max_cols = options.individual_plots_max_cols
             n_cols = _num_images if _max_cols is None else min(_num_images, _max_cols)
             n_rows = int(np.ceil(_num_images / n_cols))
 
         fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=options.figsize, sharey=options.sharey)
-        if options.tight_layout:
-            fig.tight_layout()
         fig.subplots_adjust(top=0.9)
 
         if n_rows == 1 and n_cols == 1:
             axs = [axs]
         else:
             axs = axs.reshape(-1)
 
@@ -177,14 +175,16 @@
                 if n_unique > 50:
                     options.x_ticks_rotation = 90
                 elif n_unique > 10:
                     options.x_ticks_rotation = 45
 
             self._set_ticks_rotation(ax_i, options.x_ticks_rotation, options.y_ticks_rotation)
 
+        if options.tight_layout:
+            fig.tight_layout()
         return fig
 
     def _render_kdeplot(self, df, options: KDEPlotOptions) -> plt.Figure:
 
         if options.individual_plots_key is None:
             dfs = [df]
             n_rows = 1
@@ -193,16 +193,14 @@
             dfs = [df[df[options.individual_plots_key] == key] for key in df[options.individual_plots_key].unique()]
             _num_images = len(dfs)
             _max_cols = options.individual_plots_max_cols
             n_cols = _num_images if _max_cols is None else min(_num_images, _max_cols)
             n_rows = int(np.ceil(_num_images / n_cols))
 
         fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=options.figsize, sharey=options.sharey)
-        if options.tight_layout:
-            fig.tight_layout()
         fig.subplots_adjust(top=0.9)
 
         if n_rows == 1 and n_cols == 1:
             axs = [axs]
         else:
             axs = axs.reshape(-1)
 
@@ -251,20 +249,20 @@
                 if n_unique > 50:
                     options.x_ticks_rotation = 90
                 elif n_unique > 10:
                     options.x_ticks_rotation = 45
 
             self._set_ticks_rotation(ax_i, options.x_ticks_rotation, options.y_ticks_rotation)
 
+        if options.tight_layout:
+            fig.tight_layout()
         return fig
 
     def _render_violinplot(self, df, options: ViolinPlotOptions) -> plt.Figure:
         fig, ax = plt.subplots(nrows=1, ncols=1, figsize=options.figsize)
-        if options.tight_layout:
-            fig.tight_layout()
         fig.subplots_adjust(top=0.9)
 
         plot_args = dict(
             data=df,
             x=options.x_label_key,
             y=options.y_label_key,
             ax=ax,
@@ -300,21 +298,20 @@
             n_unique = len(df[options.x_label_key].unique())
             if n_unique > 50:
                 options.x_ticks_rotation = 90
             elif n_unique > 10:
                 options.x_ticks_rotation = 45
 
         self._set_ticks_rotation(ax, options.x_ticks_rotation, options.y_ticks_rotation)
-
+        if options.tight_layout:
+            fig.tight_layout()
         return fig
 
     def _render_barplot(self, df, options: BarPlotOptions) -> plt.Figure:
         fig, ax = plt.subplots(nrows=1, ncols=1, figsize=options.figsize)
-        if options.tight_layout:
-            fig.tight_layout()
         fig.subplots_adjust(top=0.9)
 
         barplot_args = dict(
             data=df,
             x=options.x_label_key,
             width=options.width,
             ax=ax,
@@ -345,54 +342,55 @@
         ax = plot_fn(**barplot_args)
 
         ax.set_xlabel(options.x_label_name)
         ax.set_ylabel(options.y_label_name)
         if options.labels_name is not None:
             ax.legend(title=options.labels_name)
 
-        # Write the values on the graph
-        y_ticklabels_fontsize = ax.get_yticklabels()[0].get_fontsize()
-        for container in ax.containers:
-            for bar in container:
-                width = bar.get_width()
-                height = bar.get_y() + bar.get_height() / 2
-                width_rounded = round(width, 1) if width >= 0.1 else float(f"{width:.1e}")
-                ax.text(width + 0.5, height, f"{width_rounded}%", ha="left", va="center", fontsize=y_ticklabels_fontsize)
+        if options.show_values:
+            # Write the values on the graph
+            y_ticklabels_fontsize = ax.get_yticklabels()[0].get_fontsize()
+            for container in ax.containers:
+                for bar in container:
+                    width = bar.get_width()
+                    height = bar.get_y() + bar.get_height() / 2
+                    width_rounded = round(width, 1) if width >= 0.1 else float(f"{width:.1e}")
+                    ax.text(width + 0.5, height, f"{width_rounded}%", ha="left", va="center", fontsize=y_ticklabels_fontsize)
 
         if options.log_scale is True:
             ax.set_yscale("log")
             ax.set_ylabel(options.y_label_name + " (log scale)")
 
         if options.x_ticks_rotation == "auto":
             n_unique = len(df[options.x_label_key].unique())
             if n_unique > 50:
                 options.x_ticks_rotation = 90
             elif n_unique > 10:
                 options.x_ticks_rotation = 45
 
-        if options.show_values:
-            self._show_values(ax)
-
         self._set_ticks_rotation(ax, options.x_ticks_rotation, options.y_ticks_rotation)
 
+        if options.tight_layout:
+            fig.tight_layout()
         return fig
 
     def _render_heatmap(self, data: Mapping[str, np.ndarray], options: HeatmapOptions) -> plt.Figure:
 
-        fig, axes = plt.subplots(nrows=len(data), ncols=1, figsize=options.figsize, tight_layout=options.tight_layout)
+        fig, axes = plt.subplots(nrows=1, ncols=len(data), figsize=options.figsize, tight_layout=options.tight_layout)
         fig.subplots_adjust()
 
         for i, (key, heatmap) in enumerate(data.items()):
             ax = axes[i] if len(data) > 1 else axes
+            cbar = options.cbar if i + 1 == len(data) else False
             heatmap_args = dict(
                 data=heatmap,
                 xticklabels=options.xticklabels,
                 yticklabels=options.yticklabels,
                 annot=options.annot,
-                cbar=options.cbar,
+                cbar=cbar,
                 cbar_kws={"shrink": 0.5},
                 square=options.square,
                 cmap=options.cmap,
                 linewidths=0.5,
                 fmt=options.fmt,
                 ax=ax,
             )
```

## Comparing `data_gradients-0.1.1.dist-info/LICENSE.md` & `data_gradients-0.1.2.dist-info/LICENSE.md`

 * *Files identical despite different names*

## Comparing `data_gradients-0.1.1.dist-info/METADATA` & `data_gradients-0.1.2.dist-info/METADATA`

 * *Files 9% similar despite different names*

```diff
@@ -1,35 +1,36 @@
 Metadata-Version: 2.1
 Name: data-gradients
-Version: 0.1.1
+Version: 0.1.2
 Summary: DataGradients
 Home-page: https://github.com/Deci-AI/data-gradients
 Author: Deci AI
 Author-email: rnd@deci.ai
 Keywords: Deci,AI,Data,Deep Learning,Computer Vision,PyTorch
 Description-Content-Type: text/markdown
 License-File: LICENSE.md
 Requires-Dist: hydra-core (>=1.2.0)
 Requires-Dist: omegaconf (>=2.2.3)
 Requires-Dist: pygments (>=2.13.0)
 Requires-Dist: tqdm (>=4.64.1)
-Requires-Dist: appdirs (>=1.4.4)
+Requires-Dist: platformdirs (>=2.5.2)
 Requires-Dist: opencv-python
 Requires-Dist: Pillow
 Requires-Dist: tensorboard
 Requires-Dist: torch
 Requires-Dist: torchvision
 Requires-Dist: numpy
 Requires-Dist: matplotlib
 Requires-Dist: scipy
 Requires-Dist: rapidfuzz
 Requires-Dist: coverage (~=5.3.1)
 Requires-Dist: seaborn
 Requires-Dist: xhtml2pdf
 Requires-Dist: jinja2
+Requires-Dist: imagededup
 
 # DataGradients
 <div align="center">
 <p align="center">
   <a href="https://github.com/Deci-AI/super-gradients#prerequisites"><img src="https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9-blue" /></a>
   <a href="https://pypi.org/project/data-gradients/"><img src="https://img.shields.io/pypi/v/data-gradients" /></a>
   <a href="https://github.com/Deci-AI/data-gradients/releases"><img src="https://img.shields.io/github/v/release/Deci-AI/data-gradients" /></a>
@@ -57,26 +58,39 @@
 ## Features
 - Image-Level Evaluation: DataGradients evaluates key image features such as resolution, color distribution, and average brightness.
 - Class Distribution: The library extracts stats allowing you to know which classes are the most used, how many objects do you have per image, how many image without any label, ...
 - Heatmap Generation: DataGradients produces heatmaps of bounding boxes or masks, allowing you to understand if the objects are positioned in the right area.
 - And [many more](./documentation/feature_description.md)!
 
 <div align="center">
-  <img src="documentation/assets/report_image_stats.png" width="250px">
-  <img src="documentation/assets/report_mask_sample.png" width="250px">
-  <img src="documentation/assets/report_classes_distribution.png" width="250px">
+  <a href="https://github.com/Deci-AI/data-gradients/raw/master/documentation/assets/report_image_stats.png"><img src="documentation/assets/report_image_stats.png" width="250px"></a>
+  <a href="https://github.com/Deci-AI/data-gradients/raw/master/documentation/assets/report_mask_sample.png"><img src="documentation/assets/report_mask_sample.png" width="250px"></a>
+  <a href="https://github.com/Deci-AI/data-gradients/raw/master/documentation/assets/report_classes_distribution.png"><img src="documentation/assets/report_classes_distribution.png" width="250px"></a>
   <p><em>Example of pages from the Report</em>
 </div>
 
+<div align="center">
+  <a href="https://github.com/Deci-AI/data-gradients/raw/master/documentation/assets/SegmentationBoundingBoxArea.png"><img src="documentation/assets/SegmentationBoundingBoxArea.png" width="375px"></a>
+  <a href="https://github.com/Deci-AI/data-gradients/raw/master/documentation/assets/SegmentationBoundingBoxResolution.png"><img src="documentation/assets/SegmentationBoundingBoxResolution.png" width="375px"></a>
+  <br />
+  <a href="https://github.com/Deci-AI/data-gradients/raw/master/documentation/assets/SegmentationClassFrequency.png"><img src="documentation/assets/SegmentationClassFrequency.png" width="375px"></a>
+  <a href="https://github.com/Deci-AI/data-gradients/raw/master/documentation/assets/SegmentationComponentsPerImageCount.png"><img src="documentation/assets/SegmentationComponentsPerImageCount.png" width="375px"></a>
+  <p><em>Example of specific features</em>
+</div>
+
+## Examples 
+[COCO 2017 Detection report](documentation/assets/Report_COCO.pdf)
+
+[Cityscapes Segmentation report](documentation/assets/Report_CityScapes.pdf)
 
 ## Installation
 You can install DataGradients directly from the GitHub repository.
 
 ```
-pip install git+https://github.com/Deci-AI/data-gradients
+pip install data-gradients
 ```
 
 
 ## Quick Start
 
 ### Prerequisites
```

### html2text {}

```diff
@@ -1,18 +1,19 @@
-Metadata-Version: 2.1 Name: data-gradients Version: 0.1.1 Summary:
+Metadata-Version: 2.1 Name: data-gradients Version: 0.1.2 Summary:
 DataGradients Home-page: https://github.com/Deci-AI/data-gradients Author: Deci
 AI Author-email: rnd@deci.ai Keywords: Deci,AI,Data,Deep Learning,Computer
 Vision,PyTorch Description-Content-Type: text/markdown License-File: LICENSE.md
 Requires-Dist: hydra-core (>=1.2.0) Requires-Dist: omegaconf (>=2.2.3)
 Requires-Dist: pygments (>=2.13.0) Requires-Dist: tqdm (>=4.64.1) Requires-
-Dist: appdirs (>=1.4.4) Requires-Dist: opencv-python Requires-Dist: Pillow
+Dist: platformdirs (>=2.5.2) Requires-Dist: opencv-python Requires-Dist: Pillow
 Requires-Dist: tensorboard Requires-Dist: torch Requires-Dist: torchvision
 Requires-Dist: numpy Requires-Dist: matplotlib Requires-Dist: scipy Requires-
 Dist: rapidfuzz Requires-Dist: coverage (~=5.3.1) Requires-Dist: seaborn
-Requires-Dist: xhtml2pdf Requires-Dist: jinja2 # DataGradients
+Requires-Dist: xhtml2pdf Requires-Dist: jinja2 Requires-Dist: imagededup #
+DataGradients
 [https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9-blue] [https:/
    /img.shields.io/pypi/v/data-gradients] [https://img.shields.io/github/v/
     release/Deci-AI/data-gradients] [https://img.shields.io/badge/license-
                               Apache%202.0-blue]
 DataGradients is an open-source python based library specifically designed for
 computer vision dataset analysis. It automatically extracts features from your
 datasets and combines them all into a single user-friendly report. - [Features]
@@ -28,32 +29,39 @@
 any label, ... - Heatmap Generation: DataGradients produces heatmaps of
 bounding boxes or masks, allowing you to understand if the objects are
 positioned in the right area. - And [many more](./documentation/
 feature_description.md)!
      [documentation/assets/report_image_stats.png] [documentation/assets/
 report_mask_sample.png] [documentation/assets/report_classes_distribution.png]
                        Example of pages from the Report
-## Installation You can install DataGradients directly from the GitHub
-repository. ``` pip install git+https://github.com/Deci-AI/data-gradients ```
-## Quick Start ### Prerequisites - **Dataset**: Includes a **Train** set and a
-**Validation** or a **Test** set. - **Class Names**: A list of the unique
-categories present in your dataset. - **Iterable**: A method to iterate over
-your Dataset providing images and labels. Can be any of the following: -
-PyTorch Dataloader - PyTorch Dataset - Generator that yields image/label pairs
-- Any other iterable you use for model training/validation Please ensure all
-the points above are checked before you proceed with **DataGradients**. **Good
-to Know**: DataGradients will try to find out how the dataset returns images
-and labels. - If something cannot be automatically determined, you will be
-asked to provide some extra information through a text input. - In some extreme
-cases, the process will crash and invite you to implement a custom dataset
-adapter (see relevant section) **Heads up**: We currently don't provide out-of-
-the-box dataset/dataloader implementation. You can find multiple dataset
-implementations in [PyTorch](https://pytorch.org/vision/stable/datasets.html)
-or [SuperGradients](https://docs.deci.ai/super-gradients/src/super_gradients/
-training/datasets/Dataset_Setup_Instructions.html). **Example** ``` python from
+ [documentation/assets/SegmentationBoundingBoxArea.png] [documentation/assets/
+                    SegmentationBoundingBoxResolution.png]
+ [documentation/assets/SegmentationClassFrequency.png] [documentation/assets/
+                   SegmentationComponentsPerImageCount.png]
+                         Example of specific features
+## Examples [COCO 2017 Detection report](documentation/assets/Report_COCO.pdf)
+[Cityscapes Segmentation report](documentation/assets/Report_CityScapes.pdf) ##
+Installation You can install DataGradients directly from the GitHub repository.
+``` pip install data-gradients ``` ## Quick Start ### Prerequisites -
+**Dataset**: Includes a **Train** set and a **Validation** or a **Test** set. -
+**Class Names**: A list of the unique categories present in your dataset. -
+**Iterable**: A method to iterate over your Dataset providing images and
+labels. Can be any of the following: - PyTorch Dataloader - PyTorch Dataset -
+Generator that yields image/label pairs - Any other iterable you use for model
+training/validation Please ensure all the points above are checked before you
+proceed with **DataGradients**. **Good to Know**: DataGradients will try to
+find out how the dataset returns images and labels. - If something cannot be
+automatically determined, you will be asked to provide some extra information
+through a text input. - In some extreme cases, the process will crash and
+invite you to implement a custom dataset adapter (see relevant section) **Heads
+up**: We currently don't provide out-of-the-box dataset/dataloader
+implementation. You can find multiple dataset implementations in [PyTorch]
+(https://pytorch.org/vision/stable/datasets.html) or [SuperGradients](https://
+docs.deci.ai/super-gradients/src/super_gradients/training/datasets/
+Dataset_Setup_Instructions.html). **Example** ``` python from
 torchvision.datasets import CocoDetection train_data = CocoDetection(...)
 val_data = CocoDetection(...) class_names = ["person", "bicycle", "car",
 "motorcycle", ...] ``` ### Dataset Analysis You are now ready to go, chose the
 relevant analyzer for your task and run it over your datasets! **Object
 Detection** ```python from data_gradients.managers.detection_manager import
 DetectionAnalysisManager train_data = ... val_data = ... class_names = ...
 analyzer = DetectionAnalysisManager( report_title="Testing Data-Gradients
```

## Comparing `data_gradients-0.1.1.dist-info/RECORD` & `data_gradients-0.1.2.dist-info/RECORD`

 * *Files 22% similar despite different names*

```diff
@@ -1,33 +1,33 @@
-data_gradients/__init__.py,sha256=rnObPjuBcEStqSO0S6gsdS_ot8ITOQjVj_-P1LUUYpg,22
-data_gradients/requirements.txt,sha256=1Si9lFhxUftmRhT75eKHsfIB87CVGxW_h4Ow-J2P-Dc,205
+data_gradients/__init__.py,sha256=YvuYzWnKtqBb-IqG8HAu-nhIYAsgj9Vmc_b9o7vO-js,22
+data_gradients/requirements.txt,sha256=-vtVqLU6jHSCrMUQa80TPVRUa7DU7ERpob2CMpKo-6Q,221
 data_gradients/assets/__init__.py,sha256=u-dAbknZKaLTrSPCuSm7mOxVUTisTla2P9zlRp7a1nU,231
 data_gradients/assets/assets_container.py,sha256=nsY6AYtlG1TdfrF9atdYzGcqdzpqFByXG3UqV-4ODuY,2755
 data_gradients/assets/css/test.css,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-data_gradients/assets/html/basic_info_fe.html,sha256=VMx8YM_z8T8Q5gljZNgmbZlxsOcDO3olDUM2lhgH-1E,3245
+data_gradients/assets/html/basic_info_fe.html,sha256=NQEw0h3ww6gq20qK30vkJZ7O6okrV0X8R5X7IRupjyY,3244
 data_gradients/assets/html/doc_template.html,sha256=yYMAT9adkR-EP-zVIrhSxuWCyLWYDATpPleRqvQolks,8301
 data_gradients/assets/html/test.html,sha256=WUoHLRvnM8pf46wEf7OcNsQyyubQfn1JiNt2t49YvJ4,123
 data_gradients/assets/images/chart_demo.png,sha256=zlOr7jOIHD-DTAQccBOgeJ9CNeMfaqF5gvr3o3Ui0OU,51292
 data_gradients/assets/images/info.png,sha256=fowx3nTT6ZVy4SuJecN13R_SXos3Uo2v9AbM73Mr9FE,139838
 data_gradients/assets/images/logo.png,sha256=EgIIuDIL3HHWDPk_2qPD7-sFB8s1ERXrTXZI5nLG4IM,36081
 data_gradients/assets/images/warning.png,sha256=XVnDsyHU2u0Hqg5FHMmrL5orUqWTbwFNyLLxGmfogZQ,75086
 data_gradients/assets/text/lorem_ipsum.txt,sha256=V22Sg3hEbEGrGrf4s0Up6lvSyZxcY0qhVFEGFeradQM,333
 data_gradients/assets/text/test.txt,sha256=dQnlvaDHYtK6x_kNdYtbImP6Acy8VCq1498WO-CObKk,12
 data_gradients/batch_processors/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 data_gradients/batch_processors/base.py,sha256=redCxcEwMmMULvWSRjO0ne-WD_IHV8HFUMLcstwL7AM,1636
 data_gradients/batch_processors/detection.py,sha256=Qtqb6nDeNiNikglqv78TGwKx60POFaj7A7zwx7kvWgw,1125
 data_gradients/batch_processors/segmentation.py,sha256=aHVofThKhYeeh5kUxeLCLGYYkeSuagAgprwOJCis2Z0,1236
-data_gradients/batch_processors/utils.py,sha256=pA7lnrn7ikOsIOt1gk0VCQ-p_5hXF2czCoo7YyZ6jRU,1193
+data_gradients/batch_processors/utils.py,sha256=C9S-TcrycZvWmnwrO-QKu0imouqHopfa0--ddx3yhUM,1155
 data_gradients/batch_processors/adapters/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 data_gradients/batch_processors/adapters/dataset_adapter.py,sha256=dlgINiDcyBoX7DGCkoyDDhhdbETUzXWWcDVOOf3ETTw,4621
 data_gradients/batch_processors/adapters/tensor_extractor.py,sha256=ri-WRtTGCF0C5Fczd4iLGYKOU3zdKyaMPyaBvrKczyg,6698
 data_gradients/batch_processors/formatters/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 data_gradients/batch_processors/formatters/base.py,sha256=4_NXp9XjU5M4LHACcEhvuOSGb4UhwrZAutJKwB3HJGE,725
-data_gradients/batch_processors/formatters/detection.py,sha256=ukdLeGtBRQ3hs89u7wfTAG4UoYIVGiDR7fyZmyHSVs8,8757
-data_gradients/batch_processors/formatters/segmentation.py,sha256=nhugc73zkkk9UUR87NIoOr1G7sn5E_69G7vlNhoJr20,6940
+data_gradients/batch_processors/formatters/detection.py,sha256=kNTSpd5vckOtyO925-9D2z-QolOY4GNiHiKj8z2xXc0,9044
+data_gradients/batch_processors/formatters/segmentation.py,sha256=AulVeZEo5AViQpLU4tsCaCdKyptLqkDBBfq3xurgzNI,7181
 data_gradients/batch_processors/formatters/utils.py,sha256=4Mez4LUbTOw8skJ1kKrvbh4_67whn28vtzdqK_ta3A8,1912
 data_gradients/batch_processors/preprocessors/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 data_gradients/batch_processors/preprocessors/base.py,sha256=6WUenirzUy7NpxrTAaN0sAxSld04_IncVSgeq6sffZU,994
 data_gradients/batch_processors/preprocessors/contours.py,sha256=s9-cHTXOSMyRaFogaeCK3OfX75DjGumpAeE10JHeUew,5189
 data_gradients/batch_processors/preprocessors/detection.py,sha256=dyqD90Z4sC7nJonpOKf2lIYNom_gEtLEBMIYS5q21eY,2527
 data_gradients/batch_processors/preprocessors/segmentation.py,sha256=4GtACw_1DHMILLUpTFR5InVysP5YBza-fKdW8Rap3sM,1836
 data_gradients/common/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
@@ -36,72 +36,84 @@
 data_gradients/common/factories/__init__.py,sha256=T1XCVwMsm-dF9nNRxZ3ZaZE92jnpskgJIarT66qThM4,140
 data_gradients/common/factories/base_factory.py,sha256=YRXOPJ_3w3ByZDXZWrSkFzCTNsdM4cUjQczv48SrreU,3694
 data_gradients/common/factories/feature_extractors_factory.py,sha256=2P5LKfRpTpDziMg0Wm0CKXm5RCRutWhi9eoy006DmfY,206
 data_gradients/common/factories/list_factory.py,sha256=rz9LZmCxQl1MpV8JA6vLQ42zkkPO-UEw7HTJ1MPQuLQ,569
 data_gradients/common/registry/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 data_gradients/common/registry/registry.py,sha256=nUIHDdh_Ssw5AxwsYy6X4N-hjSZKQ5l8R94ntDP4h4E,1245
 data_gradients/config/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-data_gradients/config/detection.yaml,sha256=ccFCN0Jgfb_yJCG8ZFCVZEbkAQIy91FDBHttoDseqfk,705
-data_gradients/config/segmentation.yaml,sha256=UMDEtK5z4YP8_c2B4u7hb0db4d-2IzSCwOlOb3x8dSM,756
-data_gradients/config/utils.py,sha256=5DsPja7f00PA8fPieHfkQjHVyq87eMDD9vlvRyzTLBQ,4511
+data_gradients/config/detection.yaml,sha256=8byU7PluJwNGZPscs5UAj1Xp8V0l72-WeUChnuoewHM,903
+data_gradients/config/segmentation.yaml,sha256=ldKjTybVqSvRnXuFg3Lv6Ff10VYodg2G33a_X2MnQO0,954
+data_gradients/config/utils.py,sha256=KxIt36cZqP4fTE61h5Dp7U0KLK8dZtXARjgo1qK4rFo,6641
 data_gradients/config/data/__init__.py,sha256=v5G5etJhQHboiotmoFIdnex7_4cPdhPqlk4CXdSJij4,157
-data_gradients/config/data/caching_utils.py,sha256=Gj8xUzWerrpGAQHIBP8no_GOxbRXe61jTKgNynwBT9M,8713
-data_gradients/config/data/data_config.py,sha256=b3Y1bn2kph5gCv_HUCvD0sqD86sKSvzRmAofZxdzO5w,8640
+data_gradients/config/data/caching_utils.py,sha256=JDvI-hEAr94_526nyM4dH__Tmmzryj2FbNiOTjoTja8,8725
+data_gradients/config/data/data_config.py,sha256=UigT15qaxmGyEHMkt3J1UJFS0ufqUzYGCjK__DX7juA,8676
 data_gradients/config/data/questions.py,sha256=db6e95Xsx_hdLtxf0qvOKUrxevHArSPoqXD6oScTP-0,2894
-data_gradients/config/data/typing.py,sha256=3qGGuRVEB4-v1bAT_woKaQvJEwGciB2AAWsed86PVEA,231
-data_gradients/datasets/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+data_gradients/config/data/typing.py,sha256=vgfMZVxGsD7hqpAQl11Y_SeD5_sdy4A3T9JWNAFOAeQ,503
+data_gradients/datasets/FolderProcessor.py,sha256=8CrZDEKYxSqTwFIf6X93UBIyzFumJqDmxVSw-TITIuE,9524
+data_gradients/datasets/__init__.py,sha256=AkMyc4HAM0nx8ZW_sa_28euuvyEL4CYRsbRDvNsS-Iw,288
+data_gradients/datasets/base_dataset.py,sha256=PzwXmg_e693IwU25a7X9MGYWTUSCSaGQMD3DkvdpSyA,2820
 data_gradients/datasets/bdd_dataset.py,sha256=fWulH4IsYwhHCp9N0NtKLqSvAIhB7qknAoTs1cUrna0,2265
-data_gradients/feature_extractors/__init__.py,sha256=r8Yvdgww1akoYBBi_gYP7ME30ST0dkdcVjQmtEhf4Bo,1565
-data_gradients/feature_extractors/abstract_feature_extractor.py,sha256=78r0llgO8ISXUdqMHRFZuoFAqBwJuN3M_zja5ocwSjo,1176
+data_gradients/datasets/utils.py,sha256=cbUB2jrtl3Mkk7IvG5nAoOv95uZ6ha3F-iRurPdGKrY,859
+data_gradients/datasets/detection/__init__.py,sha256=zX0VltL1bEsi23JvFauVRyjyQMxMVnaTqk4QUPQuOzs,386
+data_gradients/datasets/detection/voc_detection_dataset.py,sha256=Qei7dwR0NjqJrrqVfwDAEvTCd7jBjUQEe4nS1yDKtvQ,4037
+data_gradients/datasets/detection/voc_format_detection_dataset.py,sha256=2Z8vJGwuT4wTtEQJSqsSSbFSagS8u3_NW-_QADtadTM,6583
+data_gradients/datasets/detection/yolo_format_detection_dataset.py,sha256=uQEHeQTPOg6sDyTIh90khg_a-WKejxRofURVrTTYfZA,6987
+data_gradients/feature_extractors/__init__.py,sha256=cUev5g8rXNmWs5vMoxkh_EBr4LexAbnu0ZDzArTEWTE,1605
+data_gradients/feature_extractors/abstract_feature_extractor.py,sha256=BN-eOARlX4TNmw6k6nSDYcgffB-etuo0krcbiC85WjQ,1461
 data_gradients/feature_extractors/features.py,sha256=S3x6en1Q_QBrfhIeshiI9lx1wNt4IK2r6doa_vtulCY,4445
-data_gradients/feature_extractors/utils.py,sha256=PbiZrOr1jCT-2lQcGPFSEVYQCNYgiapFTShc-6h4fwM,1750
-data_gradients/feature_extractors/common/__init__.py,sha256=ZNSdPgTGqnWqT6A_kyHbqiNe72fod8aHbtf6jCG_SEo,325
+data_gradients/feature_extractors/utils.py,sha256=AUaBEth5zIdT9l6FXjpwiR30AqBY316iyPbQ7ApHius,4256
+data_gradients/feature_extractors/common/__init__.py,sha256=g-iFBEcKIRzLOkSamnPyl5wkmQu70cfZ23A0bnPNHqo,394
 data_gradients/feature_extractors/common/heatmap.py,sha256=mKdkNbFLYM3HFsIRnjq8DUIAfLyZW5OxRVXx5E08xXI,2337
-data_gradients/feature_extractors/common/image_average_brightness.py,sha256=FfG35jpo_C6Y1Gye17LIH7WCaTWcdTKJuo7fzJjPUAk,2971
-data_gradients/feature_extractors/common/image_color_distribution.py,sha256=GclsbzhkVZZSYwAtYUKFROie3q8BoPOlDL-BZCKYEvs,4478
-data_gradients/feature_extractors/common/image_resolution.py,sha256=J0-Ijr0GRtBuGqPWJ3qaiqdbbEWKgNHutxFyaImJmxw,3382
-data_gradients/feature_extractors/common/sample_visualization.py,sha256=U1XLaMF4YMJoaUwcVQLIYQ5WHVpUNV7YJUGDxziMHhI,3219
-data_gradients/feature_extractors/common/summary.py,sha256=rXhYKZ14o8lxdD9eegmyw33covOxJdF8PbEDKCJcR_E,4930
+data_gradients/feature_extractors/common/image_average_brightness.py,sha256=W8mN1M_CoOff73FB7Tt8QRQCAhSzeQAO4sccQa-N-A0,3740
+data_gradients/feature_extractors/common/image_color_distribution.py,sha256=5_25IjxqfpjF68m0iVwvq0XY2VYG5CBbPhsVnA5BrjY,4490
+data_gradients/feature_extractors/common/image_duplicates.py,sha256=juAjzWOwvqKbSmj7nrBtvpgCwMnXfclzqmJ0oqFnLSA,13444
+data_gradients/feature_extractors/common/image_resolution.py,sha256=Sgz0_SzOrsbjuuCmow3b_iZ0HsNgF3_fYmDn-J8mkVw,3362
+data_gradients/feature_extractors/common/sample_visualization.py,sha256=JFDfHNBT_DIvyCfVnEokdt-ykYygwxkcq3_frj7q98Q,3218
+data_gradients/feature_extractors/common/summary.py,sha256=GqZWCQdYwqCml6OEsvcVUB97Fv1k3fPjp43BBA7CIMg,5016
 data_gradients/feature_extractors/object_detection/__init__.py,sha256=nMd12mIqOwLDASOI1b1NB-gHSnH6zlelt_JmguRjxRU,790
-data_gradients/feature_extractors/object_detection/bounding_boxes_area.py,sha256=3-8hFbk01elE3EOSZNBC_wuZIow-QYOS216DdAcqpGA,2757
-data_gradients/feature_extractors/object_detection/bounding_boxes_iou.py,sha256=Qjp-jkBGSfXGQhuJNIWHp_-N4z33mMSacsT3neRQ93U,5663
-data_gradients/feature_extractors/object_detection/bounding_boxes_per_image_count.py,sha256=7nVQgwi_-s7DfnjPiLI2f5e7LKXsLvtm8GtlC4vycs8,2382
+data_gradients/feature_extractors/object_detection/bounding_boxes_area.py,sha256=2ssj4c4tRBM2Qp2O2M9gZBBv5TSm6yc1uu5--lKQDwo,4124
+data_gradients/feature_extractors/object_detection/bounding_boxes_iou.py,sha256=Ip1wXiI6rdyzqqEPXmcHO32xd8hBpypd4KERlMiymXQ,5754
+data_gradients/feature_extractors/object_detection/bounding_boxes_per_image_count.py,sha256=gwvk-abBpgLWgZYCdBj6GB4e7NZYIWOOXp6s2g9FQo8,2138
 data_gradients/feature_extractors/object_detection/bounding_boxes_resolution.py,sha256=6MUtx1QKMCT-7UHgZR5JmbBPnJrjFY0_1zDDYy5RUAw,2877
-data_gradients/feature_extractors/object_detection/classes_frequency.py,sha256=c9TmN3lHrkngARzxper1UPYSEUAZXubdefU62M-1sQM,2667
-data_gradients/feature_extractors/object_detection/classes_frequency_per_image.py,sha256=qdHDz_-7r5ijqiJl-66UVF9ubW4cvsEo6WSy1ut7_bQ,2815
-data_gradients/feature_extractors/object_detection/classes_heatmap_per_class.py,sha256=OJuIW4rnz-_uxBP2v8_hCBx_fVN3wqBN4pOHfdglOuQ,2479
-data_gradients/feature_extractors/object_detection/sample_visualization.py,sha256=M8GWwSBZD9nZZ0fuy9yuX_N9pL0JueeCDMyjppGG4Kg,1971
+data_gradients/feature_extractors/object_detection/classes_frequency.py,sha256=0HylbQQ-7kUFkL9gUcqaDBV4VkYYHpvwcl4FXOSxlYY,4036
+data_gradients/feature_extractors/object_detection/classes_frequency_per_image.py,sha256=gspFSN6Z1giN3I0B8qL_yl0ckRLvaafvfkvRuDsHJqc,4232
+data_gradients/feature_extractors/object_detection/classes_heatmap_per_class.py,sha256=KfUlMRLyIEOgxxUF0fmrmUQkx2VQgtlykg2wyfQSNrQ,2596
+data_gradients/feature_extractors/object_detection/sample_visualization.py,sha256=udccey7E8SitvfT3jJSS35lqqjCwVkEG8qgpoFCP4NU,1981
 data_gradients/feature_extractors/segmentation/__init__.py,sha256=oKssDGr7mXMJKKcbML-rVxTHFGpVN-DnB9Fuqg_zRco,958
-data_gradients/feature_extractors/segmentation/bounding_boxes_area.py,sha256=pQ8op4I1NNO0IG_2LsjVeGKtk8uPak8e5kf7N9aFL_4,2776
+data_gradients/feature_extractors/segmentation/bounding_boxes_area.py,sha256=IsT4m7nm51pXxZ2HUgfwPyGfXQsT24bI9qR1F8E2pws,4209
 data_gradients/feature_extractors/segmentation/bounding_boxes_resolution.py,sha256=_CPsAUVitaD-aHL5Bn3eKxSLI4OkntOyCKZXkMIctlU,2809
-data_gradients/feature_extractors/segmentation/classes_frequency.py,sha256=Q5sUkVhCDBVj5tLyzet0txY9xGBvL9IxYTojLwQJM9c,2754
-data_gradients/feature_extractors/segmentation/classes_frequency_per_image.py,sha256=ut1UsumrnZYviSo3GAoOBpiB1RPkNxeNnhyt-7Xj3oc,2835
-data_gradients/feature_extractors/segmentation/classes_heatmap_per_class.py,sha256=pdiT3eoEC-aC8dxwtrLE8FErNovKwd2Q-jFmoWrBTQ8,2526
-data_gradients/feature_extractors/segmentation/component_frequency_per_image.py,sha256=BJShi51MymeFzIgRnjzz2jDU_zGCcvtpYnkZ93UayCU,2363
-data_gradients/feature_extractors/segmentation/components_convexity.py,sha256=x4sNzsmJNusIr070jk85XhtCQ0epG-L4DEQz1b773lE,2303
-data_gradients/feature_extractors/segmentation/components_erosion.py,sha256=oPCI6MlWuFxLSAUN34Nt0JqT7UUrWfMpMWWytmVgkvQ,3808
+data_gradients/feature_extractors/segmentation/classes_frequency.py,sha256=1QjzBEkgKwxtxPYqaPHFUjliINhm6IFiEhrl0geMrf4,4124
+data_gradients/feature_extractors/segmentation/classes_frequency_per_image.py,sha256=6Z6estaGOm0CW8uEIvWcdkCcxlPH71eazaSWwhPcKLE,4251
+data_gradients/feature_extractors/segmentation/classes_heatmap_per_class.py,sha256=31iDn2VdG9hsgUx67OtJoibENjnXGuTahk98btCaqMM,2645
+data_gradients/feature_extractors/segmentation/component_frequency_per_image.py,sha256=lCHg4-NS0fG1Iw6hEPzC2Kp0NgcZuE5r4ZY4KWHqOps,2135
+data_gradients/feature_extractors/segmentation/components_convexity.py,sha256=m9Fwl6p7gvmJvajejO-jLwge3_1ixvruTbuyS7uknys,2302
+data_gradients/feature_extractors/segmentation/components_erosion.py,sha256=RgzuXWrr0mfaiPSbc4nI4zztpNTYvei8_8C6pj52Ovs,3807
 data_gradients/feature_extractors/segmentation/sample_visualization.py,sha256=MNLQ24QaW-s0Mb9FnCT2X3YVD3pnBgZRFnyNNU0uNoA,2674
 data_gradients/managers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-data_gradients/managers/abstract_manager.py,sha256=4lUwGaJpc1Vt-aI4v8b4pWY--0DzrukDuHzMrNdNTKs,11429
-data_gradients/managers/detection_manager.py,sha256=srKW2dnH6JamOiUycC4Nd1E86fWNigSMzyo5h8n-B_M,5498
-data_gradients/managers/segmentation_manager.py,sha256=J8kEePZ7LKCPctO7iwbPgcYctnpkPAeG8ojXtoopWz8,5401
+data_gradients/managers/abstract_manager.py,sha256=LE3i3ZwcyaseoTGEU0MK0M5f7d57IUcmTtz9EREwZHM,12234
+data_gradients/managers/detection_manager.py,sha256=ClAL6wdMbl13ThfKTooJiRQqyedNEPtCTILfGZEMbGs,6065
+data_gradients/managers/segmentation_manager.py,sha256=lBOf3rA008_kSl0VGGYvX-A9U9q5JVCInb9WF5D0ZlE,5951
 data_gradients/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-data_gradients/utils/detection.py,sha256=LimFh8QXLPGw_icExR-aF5uaVqhI4ujs6h-yDgTXLcw,2760
+data_gradients/utils/detection.py,sha256=SDtVB-M5yXPidlcOyW_axlieIOwHkIE01UNTtnB4Sgs,2819
 data_gradients/utils/image_processing.py,sha256=5E2qUPoXYCFIuTgQ9IzewDeZ-Ay0u10pyQuHhaCg7Ko,1052
 data_gradients/utils/pdf_writer.py,sha256=hsL_tTCaK_3xY8YpcuG9PZhWIF9AID6IWrw-DCVSKHc,2550
 data_gradients/utils/summary_writer.py,sha256=EtlMOkaXoBLd2w9sXBeuXlNdez01e6lzcuNzlJ4-pSE,3705
 data_gradients/utils/utils.py,sha256=N5WSyuVBY2Sxm4CdEXF8OAPK0cR1-gWmhbctUr_3Wk0,2561
 data_gradients/utils/common/__init__.py,sha256=UdVptbzrZj_KxShVC3KQbzM1zLTAPDuKk0dE6KmWUiE,169
 data_gradients/utils/data_classes/__init__.py,sha256=4zkOKnF1rycWi34SXthwkqWeI1CzjTfTAWJUnLmxvQE,249
 data_gradients/utils/data_classes/contour.py,sha256=kqrU1QLpV6smKGkeXk_-sfRYEq6sHkoHXx4i3TMYvTY,260
-data_gradients/utils/data_classes/data_samples.py,sha256=0g9P6BFKOj4Tk0Fdc3er98C42qUrOescFxLpFH2SmpI,3060
+data_gradients/utils/data_classes/data_samples.py,sha256=RGDy6tewoeSrYcFZg7nCXdwF1TI2FZ7ZN-9w9apRsDE,3088
 data_gradients/utils/data_classes/extractor_results.py,sha256=f_49UZoq9SfwV6XlqeVNuztLG_PIJbTW6WX3Wz9Bv2o,3975
 data_gradients/visualize/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-data_gradients/visualize/detection.py,sha256=d25Er47jQGWO7xhI4zyPptu6HEtTF0eq-CSJvZ_D06Q,5437
-data_gradients/visualize/images.py,sha256=FD9-CtzJ5UsnB5gw3jzlO_etrbFLaWGjn_eeZBxXTLw,5092
+data_gradients/visualize/images.py,sha256=PtBdSj8tSMf5qBh05JhcFhNTh9qDXwu9nJ9tO8C9b0g,5101
 data_gradients/visualize/plot_options.py,sha256=wuEQ8lkbyjb3OWnUxf-KU8M8aQm7EP01p-tVcDjFTXs,11756
-data_gradients/visualize/seaborn_renderer.py,sha256=k7W6innVtGdArVZ4AKEYr92y-ccJiTiFv9O283meBGU,16700
-data_gradients-0.1.1.dist-info/LICENSE.md,sha256=jWCBQN-JmCX0hwvn3MqItDj18W3riP4zda31ncMkcfA,11341
-data_gradients-0.1.1.dist-info/METADATA,sha256=5B88JPCkvPeXXBPU4fHezwjjYXbNiwKXMnULtuoxiiM,9226
-data_gradients-0.1.1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-data_gradients-0.1.1.dist-info/top_level.txt,sha256=zATNcRYA5TY95dQFVVrFbhJndVZotUYDEM2v3kL_84E,15
-data_gradients-0.1.1.dist-info/RECORD,,
+data_gradients/visualize/seaborn_renderer.py,sha256=K7eD6T2geWhFKXq8KkzRWgMDX6s2bhHXDA7E1O5EniA,16753
+data_gradients/visualize/utils.py,sha256=J4jYcpZ8nKZZxG633GJOWvnXDL0BmCf2EoOp726Qsts,1165
+data_gradients/visualize/detection/__init__.py,sha256=gqN9L_Vwo5csejFyzl9p6ZxAeVlO-ZDgeCOthooarCs,96
+data_gradients/visualize/detection/detection.py,sha256=MQb-jnxAeP0wy8ookeb0Ad3l73Bpli4dUGNqgk4Awrc,4075
+data_gradients/visualize/detection/detection_legend.py,sha256=GaqfSEDOczx8xWfbSbhxFWCWZP_IG2id5nXZFb854bg,4862
+data_gradients/visualize/detection/utils.py,sha256=pI0IDThWVfVmxwQaYVk13OK_TYuSg-G_vbnZu9cKooo,1432
+data_gradients-0.1.2.dist-info/LICENSE.md,sha256=jWCBQN-JmCX0hwvn3MqItDj18W3riP4zda31ncMkcfA,11341
+data_gradients-0.1.2.dist-info/METADATA,sha256=kCr_sIa9ukLWAG1nvYj1EKFd0LiuhvaMjilSWnFbASw,10637
+data_gradients-0.1.2.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+data_gradients-0.1.2.dist-info/top_level.txt,sha256=zATNcRYA5TY95dQFVVrFbhJndVZotUYDEM2v3kL_84E,15
+data_gradients-0.1.2.dist-info/RECORD,,
```

